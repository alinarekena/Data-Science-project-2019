{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDescs=pd.read_csv(\"merged_Descs_sorted.csv\")\n",
    "data=mergedDescs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_names=pd.read_csv(\"ExternalID.csv\", header=None)\n",
    "train_names=pd.read_csv(\"TrainID.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"BCF\"] = data.apply(lambda row: 0 if row[\"logBCF\"] < 3.0 else 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['logBCF'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCF\n",
       "0    798\n",
       "1    209\n",
       "Name: BCF, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(['BCF'])['BCF'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for splitting the dataset, selected rows based on unique values in the 'Name' column using or (|)\n",
    "# unique values were chosen from original datasets provided by authors of publication\n",
    "test= data[(data['Name']==729)|\n",
    "           (data['Name']==367)|\n",
    "           (data['Name']==475)|\n",
    "           (data['Name']==951)|\n",
    "           (data['Name']==21)|\n",
    "           (data['Name']==810)|\n",
    "           (data['Name']==1009)|\n",
    "           (data['Name']==273)|\n",
    "           (data['Name']==168)|\n",
    "           (data['Name']==586)|\n",
    "           (data['Name']==594)|\n",
    "           (data['Name']==22)|\n",
    "           (data['Name']==13)|\n",
    "           (data['Name']==941)|\n",
    "           (data['Name']==4)|\n",
    "           (data['Name']==253)|\n",
    "           (data['Name']==748)|\n",
    "           (data['Name']==807)|\n",
    "           (data['Name']==23)|\n",
    "           (data['Name']==953)|\n",
    "           (data['Name']==876)|\n",
    "           (data['Name']==571)|\n",
    "           (data['Name']==600)|\n",
    "           (data['Name']==372)|\n",
    "           (data['Name']==492)|\n",
    "           (data['Name']==802)|\n",
    "           (data['Name']==3)|\n",
    "           (data['Name']==118)|\n",
    "           (data['Name']==606)|\n",
    "           (data['Name']==42)|\n",
    "           (data['Name']==569)|\n",
    "           (data['Name']==505)|\n",
    "           (data['Name']==324)|\n",
    "           (data['Name']==588)|\n",
    "           (data['Name']==173)|\n",
    "           (data['Name']==65)|\n",
    "           (data['Name']==867)|\n",
    "           (data['Name']==145)|\n",
    "           (data['Name']==1032)|\n",
    "           (data['Name']==98)|\n",
    "           (data['Name']==101)|\n",
    "           (data['Name']==478)|\n",
    "           (data['Name']==30)|\n",
    "           (data['Name']==959)|\n",
    "           (data['Name']==960)|\n",
    "           (data['Name']==396)|\n",
    "           (data['Name']==621)|\n",
    "           (data['Name']==860)|\n",
    "           (data['Name']==559)|\n",
    "           (data['Name']==100)|\n",
    "           (data['Name']==568)|\n",
    "           (data['Name']==847)|\n",
    "           (data['Name']==194)|\n",
    "           (data['Name']==120)|\n",
    "           (data['Name']==24)|\n",
    "           (data['Name']==970)|\n",
    "           (data['Name']==149)|\n",
    "           (data['Name']==99)|\n",
    "           (data['Name']==204)|\n",
    "           (data['Name']==183)|\n",
    "           (data['Name']==10)|\n",
    "           (data['Name']==652)|\n",
    "           (data['Name']==840)|\n",
    "           (data['Name']==774)|\n",
    "           (data['Name']==576)|\n",
    "           (data['Name']==496)|\n",
    "           (data['Name']==74)|\n",
    "           (data['Name']==557)|\n",
    "           (data['Name']==871)|\n",
    "           (data['Name']==1030)|\n",
    "           (data['Name']==518)|\n",
    "           (data['Name']==126)|\n",
    "           (data['Name']==964)|\n",
    "           (data['Name']==215)|\n",
    "           (data['Name']==73)|\n",
    "           (data['Name']==555)|\n",
    "           (data['Name']==412)|\n",
    "           (data['Name']==150)|\n",
    "           (data['Name']==719)|\n",
    "           (data['Name']==537)|\n",
    "           (data['Name']==612)|\n",
    "           (data['Name']==136)|\n",
    "           (data['Name']==75)|\n",
    "           (data['Name']==26)|\n",
    "           (data['Name']==707)|\n",
    "           (data['Name']==424)|\n",
    "           (data['Name']==917)|\n",
    "           (data['Name']==565)|\n",
    "           (data['Name']==79)|\n",
    "           (data['Name']==66)|\n",
    "           (data['Name']==248)|\n",
    "           (data['Name']==155)|\n",
    "           (data['Name']==237)|\n",
    "           (data['Name']==721)|\n",
    "           (data['Name']==212)|\n",
    "           (data['Name']==523)|\n",
    "           (data['Name']==541)|\n",
    "           (data['Name']==112)|\n",
    "           (data['Name']==668)|\n",
    "           (data['Name']==222)|\n",
    "           (data['Name']==940)|\n",
    "           (data['Name']==491)|\n",
    "           (data['Name']==924)|\n",
    "           (data['Name']==146)|\n",
    "           (data['Name']==64)|\n",
    "           (data['Name']==539)|\n",
    "           (data['Name']==190)|\n",
    "           (data['Name']==1007)|\n",
    "           (data['Name']==511)|\n",
    "           (data['Name']==854)|\n",
    "           (data['Name']==334)|\n",
    "           (data['Name']==617)|\n",
    "           (data['Name']==409)|\n",
    "           (data['Name']==582)|\n",
    "           (data['Name']==61)|\n",
    "           (data['Name']==156)|\n",
    "           (data['Name']==1036)|\n",
    "           (data['Name']==543)|\n",
    "           (data['Name']==706)|\n",
    "           (data['Name']==95)|\n",
    "           (data['Name']==442)|\n",
    "           (data['Name']==649)|\n",
    "           (data['Name']==989)|\n",
    "           (data['Name']==121)|\n",
    "           (data['Name']==388)|\n",
    "           (data['Name']==708)|\n",
    "           (data['Name']==616)|\n",
    "           (data['Name']==436)|\n",
    "           (data['Name']==444)|\n",
    "           (data['Name']==968)|\n",
    "           (data['Name']==734)|\n",
    "           (data['Name']==205)|\n",
    "           (data['Name']==611)|\n",
    "           (data['Name']==610)|\n",
    "           (data['Name']==92)|\n",
    "           (data['Name']==819)|\n",
    "           (data['Name']==535)|\n",
    "           (data['Name']==653)|\n",
    "           (data['Name']==1033)|\n",
    "           (data['Name']==943)|\n",
    "           (data['Name']==656)|\n",
    "           (data['Name']==288)|\n",
    "           (data['Name']==347)|\n",
    "           (data['Name']==698)|\n",
    "           (data['Name']==227)|\n",
    "           (data['Name']==337)|\n",
    "           (data['Name']==745)|\n",
    "           (data['Name']==293)|\n",
    "           (data['Name']==452)|\n",
    "           (data['Name']==609)|\n",
    "           (data['Name']==384)|\n",
    "           (data['Name']==763)|\n",
    "           (data['Name']==363)|\n",
    "           (data['Name']==368)|\n",
    "           (data['Name']==1006)|\n",
    "           (data['Name']==546)|\n",
    "           (data['Name']==137)|\n",
    "           (data['Name']==797)|\n",
    "           (data['Name']==762)|\n",
    "           (data['Name']==1018)|\n",
    "           (data['Name']==153)|\n",
    "           (data['Name']==769)|\n",
    "           (data['Name']==312)|\n",
    "           (data['Name']==1020)|\n",
    "           (data['Name']==216)|\n",
    "           (data['Name']==151)|\n",
    "           (data['Name']==815)|\n",
    "           (data['Name']==800)|\n",
    "           (data['Name']==290)|\n",
    "           (data['Name']==345)|\n",
    "           (data['Name']==371)|\n",
    "           (data['Name']==401)|\n",
    "           (data['Name']==733)|\n",
    "           (data['Name']==768)|\n",
    "           (data['Name']==40)|\n",
    "           (data['Name']==58)|\n",
    "           (data['Name']==948)|\n",
    "           (data['Name']==132)|\n",
    "           (data['Name']==728)|\n",
    "           (data['Name']==676)|\n",
    "           (data['Name']==855)|\n",
    "           (data['Name']==325)|\n",
    "           (data['Name']==737)|\n",
    "           (data['Name']==309)|\n",
    "           (data['Name']==355)|\n",
    "           (data['Name']==925)|\n",
    "           (data['Name']==8)|\n",
    "           (data['Name']==743)|\n",
    "           (data['Name']==913)|\n",
    "           (data['Name']==782)|\n",
    "           (data['Name']==428)|\n",
    "           (data['Name']==301)|\n",
    "           (data['Name']==431)|\n",
    "           (data['Name']==592)|\n",
    "           (data['Name']==15)|\n",
    "           (data['Name']==903)|\n",
    "           (data['Name']==276)|\n",
    "           (data['Name']==315)|\n",
    "           (data['Name']==302)|\n",
    "           (data['Name']==525)|\n",
    "           (data['Name']==316)|\n",
    "           (data['Name']==35)|\n",
    "           (data['Name']==422)|\n",
    "           (data['Name']==723)|\n",
    "           (data['Name']==59)|\n",
    "           (data['Name']==1021)|\n",
    "           (data['Name']==184)|\n",
    "           (data['Name']==53)|\n",
    "           (data['Name']==892)|\n",
    "           (data['Name']==69)|\n",
    "           (data['Name']==973)|\n",
    "           (data['Name']==322)|\n",
    "           (data['Name']==310)|\n",
    "           (data['Name']==623)|\n",
    "           (data['Name']==965)|\n",
    "           (data['Name']==124)|\n",
    "           (data['Name']==420)|\n",
    "           (data['Name']==16)|\n",
    "           (data['Name']==882)|\n",
    "           (data['Name']==961)|\n",
    "           (data['Name']==90)|\n",
    "           (data['Name']==283)|\n",
    "           (data['Name']==839)|\n",
    "           (data['Name']==39)|\n",
    "           (data['Name']==856)|\n",
    "           (data['Name']==752)|\n",
    "           (data['Name']==1035)|\n",
    "           (data['Name']==540)|\n",
    "           (data['Name']==1004)|\n",
    "           (data['Name']==545)|\n",
    "           (data['Name']==863)|\n",
    "           (data['Name']==296)|\n",
    "           (data['Name']==447)|\n",
    "           (data['Name']==558)|\n",
    "           (data['Name']==286)|\n",
    "           (data['Name']==348)|\n",
    "           (data['Name']==111)|\n",
    "           (data['Name']==831)|\n",
    "           (data['Name']==130)|\n",
    "           (data['Name']==710)|\n",
    "           (data['Name']==56)|\n",
    "           (data['Name']==242)|\n",
    "           (data['Name']==779)|\n",
    "           (data['Name']==643)|\n",
    "           (data['Name']==574)|\n",
    "           (data['Name']==672)|\n",
    "           (data['Name']==356)|\n",
    "           (data['Name']==2)|\n",
    "           (data['Name']==147)|\n",
    "           (data['Name']==317)|\n",
    "           (data['Name']==502)|\n",
    "           (data['Name']==853)|\n",
    "           (data['Name']==932)|\n",
    "           (data['Name']==937)|\n",
    "           (data['Name']==437)|\n",
    "           (data['Name']==143)|\n",
    "           (data['Name']==922)|\n",
    "           (data['Name']==773)|\n",
    "           (data['Name']==659)|\n",
    "           (data['Name']==378)|\n",
    "           (data['Name']==230)|\n",
    "           (data['Name']==241)|\n",
    "           (data['Name']==603)|\n",
    "           (data['Name']==343)|\n",
    "           (data['Name']==524)|\n",
    "           (data['Name']==28)|\n",
    "           (data['Name']==307)|\n",
    "           (data['Name']==259)|\n",
    "           (data['Name']==342)|\n",
    "           (data['Name']==454)|\n",
    "           (data['Name']==791)|\n",
    "           (data['Name']==81)|\n",
    "           (data['Name']==697)|\n",
    "           (data['Name']==209)|\n",
    "           (data['Name']==415)|\n",
    "           (data['Name']==125)|\n",
    "           (data['Name']==599)|\n",
    "           (data['Name']==662)|\n",
    "           (data['Name']==593)|\n",
    "           (data['Name']==313)|\n",
    "           (data['Name']==628)|\n",
    "           (data['Name']==664)|\n",
    "           (data['Name']==705)|\n",
    "           (data['Name']==117)|\n",
    "           (data['Name']==760)|\n",
    "           (data['Name']==878)|\n",
    "           (data['Name']==534)|\n",
    "           (data['Name']==232)|\n",
    "           (data['Name']==234)|\n",
    "           (data['Name']==977)|\n",
    "           (data['Name']==318)|\n",
    "           (data['Name']==688)|\n",
    "           (data['Name']==104)|\n",
    "           (data['Name']==969)|\n",
    "           (data['Name']==292)|\n",
    "           (data['Name']==445)|\n",
    "           (data['Name']==482)|\n",
    "           (data['Name']==678)|\n",
    "           (data['Name']==618)|\n",
    "           (data['Name']==266)|\n",
    "           (data['Name']==642)|\n",
    "           (data['Name']==140)|\n",
    "           (data['Name']==249)|\n",
    "           (data['Name']==873)|\n",
    "           (data['Name']==984)|\n",
    "           (data['Name']==421)|\n",
    "           (data['Name']==279)|\n",
    "           (data['Name']==185)|\n",
    "           (data['Name']==758)|\n",
    "           (data['Name']==201)|\n",
    "           (data['Name']==682)|\n",
    "           (data['Name']==211)|\n",
    "           (data['Name']==1029)|\n",
    "           (data['Name']==981)|\n",
    "           (data['Name']==887)|\n",
    "           (data['Name']==133)|\n",
    "           (data['Name']==391)|\n",
    "           (data['Name']==684)|\n",
    "           (data['Name']==670)|\n",
    "           (data['Name']==694)|\n",
    "           (data['Name']==641)|\n",
    "           (data['Name']==247)|\n",
    "           (data['Name']==256)|\n",
    "           (data['Name']==686)|\n",
    "           (data['Name']==218)|\n",
    "           (data['Name']==893)|\n",
    "           (data['Name']==875)|\n",
    "           (data['Name']==18)|\n",
    "           (data['Name']==679)|\n",
    "           (data['Name']==280)|\n",
    "           (data['Name']==999)|\n",
    "           (data['Name']==323)|\n",
    "           (data['Name']==687)|\n",
    "           (data['Name']==683)\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same approach as for the test set above\n",
    "train=data[(data['Name']==485)|\n",
    "           (data['Name']==381)|\n",
    "        (data['Name']==282)|\n",
    "        (data['Name']==333)|\n",
    "        (data['Name']==275)|\n",
    "        (data['Name']==494)|\n",
    "        (data['Name']==161)|\n",
    "        (data['Name']==584)|\n",
    "        (data['Name']==823)|\n",
    "        (data['Name']==410)|\n",
    "        (data['Name']==352)|\n",
    "        (data['Name']==908)|\n",
    "        (data['Name']==261)|\n",
    "        (data['Name']==715)|\n",
    "        (data['Name']==551)|\n",
    "        (data['Name']==597)|\n",
    "        (data['Name']==711)|\n",
    "        (data['Name']==716)|\n",
    "        (data['Name']==846)|\n",
    "        (data['Name']==103)|\n",
    "        (data['Name']==224)|\n",
    "        (data['Name']==54)|\n",
    "        (data['Name']==792)|\n",
    "        (data['Name']==1)|\n",
    "        (data['Name']==457)|\n",
    "        (data['Name']==730)|\n",
    "        (data['Name']==829)|\n",
    "        (data['Name']==869)|\n",
    "        (data['Name']==219)|\n",
    "        (data['Name']==601)|\n",
    "        (data['Name']==105)|\n",
    "        (data['Name']==37)|\n",
    "        (data['Name']==148)|\n",
    "        (data['Name']==532)|\n",
    "        (data['Name']==416)|\n",
    "        (data['Name']==509)|\n",
    "        (data['Name']==453)|\n",
    "        (data['Name']==468)|\n",
    "        (data['Name']==179)|\n",
    "        (data['Name']==154)|\n",
    "        (data['Name']==615)|\n",
    "        (data['Name']==771)|\n",
    "        (data['Name']==14)|\n",
    "        (data['Name']==195)|\n",
    "        (data['Name']==265)|\n",
    "        (data['Name']==640)|\n",
    "        (data['Name']==129)|\n",
    "        (data['Name']==335)|\n",
    "        (data['Name']==455)|\n",
    "        (data['Name']==477)|\n",
    "        (data['Name']==498)|\n",
    "        (data['Name']==573)|\n",
    "        (data['Name']==1025)|\n",
    "        (data['Name']==199)|\n",
    "        (data['Name']==134)|\n",
    "        (data['Name']==91)|\n",
    "        (data['Name']==268)|\n",
    "        (data['Name']==197)|\n",
    "        (data['Name']==515)|\n",
    "        (data['Name']==80)|\n",
    "        (data['Name']==508)|\n",
    "        (data['Name']==838)|\n",
    "        (data['Name']==461)|\n",
    "        (data['Name']==722)|\n",
    "        (data['Name']==203)|\n",
    "        (data['Name']==604)|\n",
    "        (data['Name']==41)|\n",
    "        (data['Name']==11)|\n",
    "        (data['Name']==48)|\n",
    "        (data['Name']==872)|\n",
    "        (data['Name']==198)|\n",
    "        (data['Name']==735)|\n",
    "        (data['Name']==229)|\n",
    "        (data['Name']==906)|\n",
    "        (data['Name']==390)|\n",
    "        (data['Name']==581)|\n",
    "        (data['Name']==598)|\n",
    "        (data['Name']==1031)|\n",
    "        (data['Name']==221)|\n",
    "        (data['Name']==107)|\n",
    "        (data['Name']==57)|\n",
    "        (data['Name']==258)|\n",
    "        (data['Name']==171)|\n",
    "        (data['Name']==693)|\n",
    "        (data['Name']==645)|\n",
    "        (data['Name']==31)|\n",
    "        (data['Name']==884)|\n",
    "        (data['Name']==210)|\n",
    "        (data['Name']==563)|\n",
    "        (data['Name']==650)|\n",
    "        (data['Name']==361)|\n",
    "        (data['Name']==364)|\n",
    "        (data['Name']==828)|\n",
    "        (data['Name']==12)|\n",
    "        (data['Name']==564)|\n",
    "        (data['Name']==174)|\n",
    "        (data['Name']==536)|\n",
    "        (data['Name']==97)|\n",
    "        (data['Name']==519)|\n",
    "        (data['Name']==483)|\n",
    "        (data['Name']==963)|\n",
    "        (data['Name']==277)|\n",
    "        (data['Name']==583)|\n",
    "        (data['Name']==552)|\n",
    "        (data['Name']==514)|\n",
    "        (data['Name']==119)|\n",
    "        (data['Name']==304)|\n",
    "        (data['Name']==362)|\n",
    "        (data['Name']==213)|\n",
    "        (data['Name']==71)|\n",
    "        (data['Name']==231)|\n",
    "        (data['Name']==51)|\n",
    "        (data['Name']==942)|\n",
    "        (data['Name']==1019)|\n",
    "        (data['Name']==113)|\n",
    "        (data['Name']==660)|\n",
    "        (data['Name']==510)|\n",
    "        (data['Name']==142)|\n",
    "        (data['Name']==47)|\n",
    "        (data['Name']==966)|\n",
    "        (data['Name']==985)|\n",
    "        (data['Name']==157)|\n",
    "        (data['Name']==17)|\n",
    "        (data['Name']==46)|\n",
    "        (data['Name']==799)|\n",
    "        (data['Name']==544)|\n",
    "        (data['Name']==139)|\n",
    "        (data['Name']==400)|\n",
    "        (data['Name']==62)|\n",
    "        (data['Name']==252)|\n",
    "        (data['Name']==786)|\n",
    "        (data['Name']==820)|\n",
    "        (data['Name']==251)|\n",
    "        (data['Name']==862)|\n",
    "        (data['Name']==935)|\n",
    "        (data['Name']==671)|\n",
    "        (data['Name']==624)|\n",
    "        (data['Name']==633)|\n",
    "        (data['Name']==978)|\n",
    "        (data['Name']==383)|\n",
    "        (data['Name']==516)|\n",
    "        (data['Name']==891)|\n",
    "        (data['Name']==931)|\n",
    "        (data['Name']==178)|\n",
    "        (data['Name']==202)|\n",
    "        (data['Name']==562)|\n",
    "        (data['Name']==625)|\n",
    "        (data['Name']==60)|\n",
    "        (data['Name']==96)|\n",
    "        (data['Name']==920)|\n",
    "        (data['Name']==547)|\n",
    "        (data['Name']==351)|\n",
    "        (data['Name']==635)|\n",
    "        (data['Name']==776)|\n",
    "        (data['Name']==228)|\n",
    "        (data['Name']==45)|\n",
    "        (data['Name']==88)|\n",
    "        (data['Name']==192)|\n",
    "        (data['Name']==44)|\n",
    "        (data['Name']==549)|\n",
    "        (data['Name']==70)|\n",
    "        (data['Name']==499)|\n",
    "        (data['Name']==67)|\n",
    "        (data['Name']==696)|\n",
    "        (data['Name']==602)|\n",
    "        (data['Name']==36)|\n",
    "        (data['Name']==952)|\n",
    "        (data['Name']==975)|\n",
    "        (data['Name']==848)|\n",
    "        (data['Name']==440)|\n",
    "        (data['Name']==1005)|\n",
    "        (data['Name']==94)|\n",
    "        (data['Name']==550)|\n",
    "        (data['Name']==837)|\n",
    "        (data['Name']==1015)|\n",
    "        (data['Name']==507)|\n",
    "        (data['Name']==780)|\n",
    "        (data['Name']==34)|\n",
    "        (data['Name']==527)|\n",
    "        (data['Name']==407)|\n",
    "        (data['Name']==858)|\n",
    "        (data['Name']==577)|\n",
    "        (data['Name']==956)|\n",
    "        (data['Name']==667)|\n",
    "        (data['Name']==1022)|\n",
    "        (data['Name']==162)|\n",
    "        (data['Name']==620)|\n",
    "        (data['Name']==651)|\n",
    "        (data['Name']==830)|\n",
    "        (data['Name']==513)|\n",
    "        (data['Name']==865)|\n",
    "        (data['Name']==188)|\n",
    "        (data['Name']==298)|\n",
    "        (data['Name']==626)|\n",
    "        (data['Name']==548)|\n",
    "        (data['Name']==303)|\n",
    "        (data['Name']==84)|\n",
    "        (data['Name']==947)|\n",
    "        (data['Name']==566)|\n",
    "        (data['Name']==954)|\n",
    "        (data['Name']==506)|\n",
    "        (data['Name']==267)|\n",
    "        (data['Name']==319)|\n",
    "        (data['Name']==353)|\n",
    "        (data['Name']==929)|\n",
    "        (data['Name']==83)|\n",
    "        (data['Name']==833)|\n",
    "        (data['Name']==9)|\n",
    "        (data['Name']==967)|\n",
    "        (data['Name']==880)|\n",
    "        (data['Name']==957)|\n",
    "        (data['Name']==27)|\n",
    "        (data['Name']==822)|\n",
    "        (data['Name']==979)|\n",
    "        (data['Name']==332)|\n",
    "        (data['Name']==1003)|\n",
    "        (data['Name']==556)|\n",
    "        (data['Name']==843)|\n",
    "        (data['Name']==888)|\n",
    "        (data['Name']==5)|\n",
    "        (data['Name']==533)|\n",
    "        (data['Name']==330)|\n",
    "        (data['Name']==781)|\n",
    "        (data['Name']==927)|\n",
    "        (data['Name']==526)|\n",
    "        (data['Name']==25)|\n",
    "        (data['Name']==114)|\n",
    "        (data['Name']==945)|\n",
    "        (data['Name']==834)|\n",
    "        (data['Name']==764)|\n",
    "        (data['Name']==859)|\n",
    "        (data['Name']==223)|\n",
    "        (data['Name']==375)|\n",
    "        (data['Name']==1010)|\n",
    "        (data['Name']==832)|\n",
    "        (data['Name']==425)|\n",
    "        (data['Name']==634)|\n",
    "        (data['Name']==704)|\n",
    "        (data['Name']==844)|\n",
    "        (data['Name']==87)|\n",
    "        (data['Name']==877)|\n",
    "        (data['Name']==20)|\n",
    "        (data['Name']==852)|\n",
    "        (data['Name']==1001)|\n",
    "        (data['Name']==938)|\n",
    "        (data['Name']==495)|\n",
    "        (data['Name']==665)|\n",
    "        (data['Name']==427)|\n",
    "        (data['Name']==845)|\n",
    "        (data['Name']==135)|\n",
    "        (data['Name']==585)|\n",
    "        (data['Name']==365)|\n",
    "        (data['Name']==918)|\n",
    "        (data['Name']==187)|\n",
    "        (data['Name']==169)|\n",
    "        (data['Name']==474)|\n",
    "        (data['Name']==619)|\n",
    "        (data['Name']==939)|\n",
    "        (data['Name']==567)|\n",
    "        (data['Name']==666)|\n",
    "        (data['Name']==864)|\n",
    "        (data['Name']==486)|\n",
    "        (data['Name']==914)|\n",
    "        (data['Name']==622)|\n",
    "        (data['Name']==849)|\n",
    "        (data['Name']==836)|\n",
    "        (data['Name']==614)|\n",
    "        (data['Name']==467)|\n",
    "        (data['Name']==340)|\n",
    "        (data['Name']==759)|\n",
    "        (data['Name']==481)|\n",
    "        (data['Name']==374)|\n",
    "        (data['Name']==1008)|\n",
    "        (data['Name']==77)|\n",
    "        (data['Name']==958)|\n",
    "        (data['Name']==189)|\n",
    "        (data['Name']==528)|\n",
    "        (data['Name']==1012)|\n",
    "        (data['Name']==226)|\n",
    "        (data['Name']==466)|\n",
    "        (data['Name']==1002)|\n",
    "        (data['Name']==419)|\n",
    "        (data['Name']==257)|\n",
    "        (data['Name']==271)|\n",
    "        (data['Name']==373)|\n",
    "        (data['Name']==393)|\n",
    "        (data['Name']==191)|\n",
    "        (data['Name']==572)|\n",
    "        (data['Name']==49)|\n",
    "        (data['Name']==287)|\n",
    "        (data['Name']==857)|\n",
    "        (data['Name']==472)|\n",
    "        (data['Name']==811)|\n",
    "        (data['Name']==790)|\n",
    "        (data['Name']==850)|\n",
    "        (data['Name']==934)|\n",
    "        (data['Name']==661)|\n",
    "        (data['Name']==784)|\n",
    "        (data['Name']==632)|\n",
    "        (data['Name']==955)|\n",
    "        (data['Name']==200)|\n",
    "        (data['Name']==55)|\n",
    "        (data['Name']==736)|\n",
    "        (data['Name']==928)|\n",
    "        (data['Name']==68)|\n",
    "        (data['Name']==43)|\n",
    "        (data['Name']==500)|\n",
    "        (data['Name']==1034)|\n",
    "        (data['Name']==180)|\n",
    "        (data['Name']==439)|\n",
    "        (data['Name']==497)|\n",
    "        (data['Name']==264)|\n",
    "        (data['Name']==450)|\n",
    "        (data['Name']==269)|\n",
    "        (data['Name']==731)|\n",
    "        (data['Name']==911)|\n",
    "        (data['Name']==403)|\n",
    "        (data['Name']==542)|\n",
    "        (data['Name']==1013)|\n",
    "        (data['Name']==673)|\n",
    "        (data['Name']==115)|\n",
    "        (data['Name']==561)|\n",
    "        (data['Name']==122)|\n",
    "        (data['Name']==331)|\n",
    "        (data['Name']==578)|\n",
    "        (data['Name']==405)|\n",
    "        (data['Name']==714)|\n",
    "        (data['Name']==812)|\n",
    "        (data['Name']==742)|\n",
    "        (data['Name']==423)|\n",
    "        (data['Name']==756)|\n",
    "        (data['Name']==321)|\n",
    "        (data['Name']==746)|\n",
    "        (data['Name']==208)|\n",
    "        (data['Name']==738)|\n",
    "        (data['Name']==278)|\n",
    "        (data['Name']==285)|\n",
    "        (data['Name']==306)|\n",
    "        (data['Name']==327)|\n",
    "        (data['Name']==350)|\n",
    "        (data['Name']==357)|\n",
    "        (data['Name']==379)|\n",
    "        (data['Name']==399)|\n",
    "        (data['Name']==433)|\n",
    "        (data['Name']==448)|\n",
    "        (data['Name']==744)|\n",
    "        (data['Name']==750)|\n",
    "        (data['Name']==821)|\n",
    "        (data['Name']==1016)|\n",
    "        (data['Name']==910)|\n",
    "        (data['Name']==933)|\n",
    "        (data['Name']==596)|\n",
    "        (data['Name']==675)|\n",
    "        (data['Name']==490)|\n",
    "        (data['Name']==349)|\n",
    "        (data['Name']==1028)|\n",
    "        (data['Name']==554)|\n",
    "        (data['Name']==712)|\n",
    "        (data['Name']==504)|\n",
    "        (data['Name']==813)|\n",
    "        (data['Name']==1024)|\n",
    "        (data['Name']==976)|\n",
    "        (data['Name']==72)|\n",
    "        (data['Name']==426)|\n",
    "        (data['Name']==108)|\n",
    "        (data['Name']==255)|\n",
    "        (data['Name']==326)|\n",
    "        (data['Name']==339)|\n",
    "        (data['Name']==449)|\n",
    "        (data['Name']==469)|\n",
    "        (data['Name']==971)|\n",
    "        (data['Name']==1017)|\n",
    "        (data['Name']==946)|\n",
    "        (data['Name']==575)|\n",
    "        (data['Name']==131)|\n",
    "        (data['Name']==487)|\n",
    "        (data['Name']==438)|\n",
    "        (data['Name']==669)|\n",
    "        (data['Name']==700)|\n",
    "        (data['Name']==329)|\n",
    "        (data['Name']==775)|\n",
    "        (data['Name']==842)|\n",
    "        (data['Name']==522)|\n",
    "        (data['Name']==972)|\n",
    "        (data['Name']==1014)|\n",
    "        (data['Name']==883)|\n",
    "        (data['Name']==912)|\n",
    "        (data['Name']==233)|\n",
    "        (data['Name']==717)|\n",
    "        (data['Name']==186)|\n",
    "        (data['Name']==32)|\n",
    "        (data['Name']==629)|\n",
    "        (data['Name']==370)|\n",
    "        (data['Name']==489)|\n",
    "        (data['Name']==354)|\n",
    "        (data['Name']==727)|\n",
    "        (data['Name']==291)|\n",
    "        (data['Name']==1026)|\n",
    "        (data['Name']==464)|\n",
    "        (data['Name']==236)|\n",
    "        (data['Name']==369)|\n",
    "        (data['Name']==754)|\n",
    "        (data['Name']==789)|\n",
    "        (data['Name']==962)|\n",
    "        (data['Name']==785)|\n",
    "        (data['Name']==926)|\n",
    "        (data['Name']==553)|\n",
    "        (data['Name']==359)|\n",
    "        (data['Name']==206)|\n",
    "        (data['Name']==827)|\n",
    "        (data['Name']==909)|\n",
    "        (data['Name']==320)|\n",
    "        (data['Name']==560)|\n",
    "        (data['Name']==116)|\n",
    "        (data['Name']==709)|\n",
    "        (data['Name']==414)|\n",
    "        (data['Name']==644)|\n",
    "        (data['Name']==605)|\n",
    "        (data['Name']==907)|\n",
    "        (data['Name']==806)|\n",
    "        (data['Name']==591)|\n",
    "        (data['Name']==479)|\n",
    "        (data['Name']==608)|\n",
    "        (data['Name']==788)|\n",
    "        (data['Name']==50)|\n",
    "        (data['Name']==757)|\n",
    "        (data['Name']==93)|\n",
    "        (data['Name']==109)|\n",
    "        (data['Name']==377)|\n",
    "        (data['Name']==127)|\n",
    "        (data['Name']==366)|\n",
    "        (data['Name']==861)|\n",
    "        (data['Name']==681)|\n",
    "        (data['Name']==801)|\n",
    "        (data['Name']==851)|\n",
    "        (data['Name']==930)|\n",
    "        (data['Name']==803)|\n",
    "        (data['Name']==76)|\n",
    "        (data['Name']==870)|\n",
    "        (data['Name']==63)|\n",
    "        (data['Name']==33)|\n",
    "        (data['Name']==777)|\n",
    "        (data['Name']==986)|\n",
    "        (data['Name']==1023)|\n",
    "        (data['Name']==284)|\n",
    "        (data['Name']==718)|\n",
    "        (data['Name']==724)|\n",
    "        (data['Name']==295)|\n",
    "        (data['Name']==783)|\n",
    "        (data['Name']==905)|\n",
    "        (data['Name']==476)|\n",
    "        (data['Name']==772)|\n",
    "        (data['Name']==809)|\n",
    "        (data['Name']==835)|\n",
    "        (data['Name']==164)|\n",
    "        (data['Name']==761)|\n",
    "        (data['Name']==825)|\n",
    "        (data['Name']==235)|\n",
    "        (data['Name']==488)|\n",
    "        (data['Name']==82)|\n",
    "        (data['Name']==473)|\n",
    "        (data['Name']==392)|\n",
    "        (data['Name']==896)|\n",
    "        (data['Name']==281)|\n",
    "        (data['Name']==299)|\n",
    "        (data['Name']==7)|\n",
    "        (data['Name']==607)|\n",
    "        (data['Name']==658)|\n",
    "        (data['Name']==382)|\n",
    "        (data['Name']==314)|\n",
    "        (data['Name']==170)|\n",
    "        (data['Name']==794)|\n",
    "        (data['Name']==720)|\n",
    "        (data['Name']==1011)|\n",
    "        (data['Name']==459)|\n",
    "        (data['Name']==990)|\n",
    "        (data['Name']==879)|\n",
    "        (data['Name']==732)|\n",
    "        (data['Name']==78)|\n",
    "        (data['Name']==787)|\n",
    "        (data['Name']==86)|\n",
    "        (data['Name']==916)|\n",
    "        (data['Name']==176)|\n",
    "        (data['Name']==297)|\n",
    "        (data['Name']==766)|\n",
    "        (data['Name']==531)|\n",
    "        (data['Name']==796)|\n",
    "        (data['Name']==1027)|\n",
    "        (data['Name']==982)|\n",
    "        (data['Name']==866)|\n",
    "        (data['Name']==336)|\n",
    "        (data['Name']==165)|\n",
    "        (data['Name']==740)|\n",
    "        (data['Name']==239)|\n",
    "        (data['Name']==646)|\n",
    "        (data['Name']==85)|\n",
    "        (data['Name']==755)|\n",
    "        (data['Name']==451)|\n",
    "        (data['Name']==220)|\n",
    "        (data['Name']==713)|\n",
    "        (data['Name']==418)|\n",
    "        (data['Name']==983)|\n",
    "        (data['Name']==380)|\n",
    "        (data['Name']==590)|\n",
    "        (data['Name']==432)|\n",
    "        (data['Name']==570)|\n",
    "        (data['Name']==463)|\n",
    "        (data['Name']==595)|\n",
    "        (data['Name']==529)|\n",
    "        (data['Name']==770)|\n",
    "        (data['Name']==395)|\n",
    "        (data['Name']==895)|\n",
    "        (data['Name']==214)|\n",
    "        (data['Name']==89)|\n",
    "        (data['Name']==793)|\n",
    "        (data['Name']==739)|\n",
    "        (data['Name']==741)|\n",
    "        (data['Name']==753)|\n",
    "        (data['Name']==460)|\n",
    "        (data['Name']==874)|\n",
    "        (data['Name']==484)|\n",
    "        (data['Name']==358)|\n",
    "        (data['Name']==29)|\n",
    "        (data['Name']==638)|\n",
    "        (data['Name']==346)|\n",
    "        (data['Name']==881)|\n",
    "        (data['Name']==376)|\n",
    "        (data['Name']==446)|\n",
    "        (data['Name']==38)|\n",
    "        (data['Name']==995)|\n",
    "        (data['Name']==949)|\n",
    "        (data['Name']==919)|\n",
    "        (data['Name']==167)|\n",
    "        (data['Name']==805)|\n",
    "        (data['Name']==980)|\n",
    "        (data['Name']==458)|\n",
    "        (data['Name']==580)|\n",
    "        (data['Name']==394)|\n",
    "        (data['Name']==443)|\n",
    "        (data['Name']==398)|\n",
    "        (data['Name']==886)|\n",
    "        (data['Name']==996)|\n",
    "        (data['Name']==465)|\n",
    "        (data['Name']==244)|\n",
    "        (data['Name']==703)|\n",
    "        (data['Name']==841)|\n",
    "        (data['Name']==138)|\n",
    "        (data['Name']==207)|\n",
    "        (data['Name']==19)|\n",
    "        (data['Name']==480)|\n",
    "        (data['Name']==411)|\n",
    "        (data['Name']==102)|\n",
    "        (data['Name']==663)|\n",
    "        (data['Name']==904)|\n",
    "        (data['Name']==767)|\n",
    "        (data['Name']==579)|\n",
    "        (data['Name']==974)|\n",
    "        (data['Name']==587)|\n",
    "        (data['Name']==344)|\n",
    "        (data['Name']==123)|\n",
    "        (data['Name']==441)|\n",
    "        (data['Name']==402)|\n",
    "        (data['Name']==814)|\n",
    "        (data['Name']==462)|\n",
    "        (data['Name']==921)|\n",
    "        (data['Name']==385)|\n",
    "        (data['Name']==778)|\n",
    "        (data['Name']==950)|\n",
    "        (data['Name']==655)|\n",
    "        (data['Name']==701)|\n",
    "        (data['Name']==795)|\n",
    "        (data['Name']==816)|\n",
    "        (data['Name']==181)|\n",
    "        (data['Name']==901)|\n",
    "        (data['Name']==936)|\n",
    "        (data['Name']==456)|\n",
    "        (data['Name']==885)|\n",
    "        (data['Name']==196)|\n",
    "        (data['Name']==294)|\n",
    "        (data['Name']==141)|\n",
    "        (data['Name']==408)|\n",
    "        (data['Name']==900)|\n",
    "        (data['Name']==630)|\n",
    "        (data['Name']==1000)|\n",
    "        (data['Name']==387)|\n",
    "        (data['Name']==152)|\n",
    "        (data['Name']==413)|\n",
    "        (data['Name']==915)|\n",
    "        (data['Name']==699)|\n",
    "        (data['Name']==890)|\n",
    "        (data['Name']==818)|\n",
    "        (data['Name']==128)|\n",
    "        (data['Name']==328)|\n",
    "        (data['Name']==270)|\n",
    "        (data['Name']==308)|\n",
    "        (data['Name']==749)|\n",
    "        (data['Name']==804)|\n",
    "        (data['Name']==631)|\n",
    "        (data['Name']==674)|\n",
    "        (data['Name']==889)|\n",
    "        (data['Name']==589)|\n",
    "        (data['Name']==144)|\n",
    "        (data['Name']==225)|\n",
    "        (data['Name']==158)|\n",
    "        (data['Name']==289)|\n",
    "        (data['Name']==654)|\n",
    "        (data['Name']==826)|\n",
    "        (data['Name']==272)|\n",
    "        (data['Name']==52)|\n",
    "        (data['Name']==637)|\n",
    "        (data['Name']==691)|\n",
    "        (data['Name']==689)|\n",
    "        (data['Name']==520)|\n",
    "        (data['Name']==657)|\n",
    "        (data['Name']==471)|\n",
    "        (data['Name']==166)|\n",
    "        (data['Name']==300)|\n",
    "        (data['Name']==765)|\n",
    "        (data['Name']==987)|\n",
    "        (data['Name']==639)|\n",
    "        (data['Name']==944)|\n",
    "        (data['Name']==994)|\n",
    "        (data['Name']==274)|\n",
    "        (data['Name']==254)|\n",
    "        (data['Name']==159)|\n",
    "        (data['Name']==240)|\n",
    "        (data['Name']==106)|\n",
    "        (data['Name']==530)|\n",
    "        (data['Name']==868)|\n",
    "        (data['Name']==429)|\n",
    "        (data['Name']==260)|\n",
    "        (data['Name']==417)|\n",
    "        (data['Name']==389)|\n",
    "        (data['Name']==690)|\n",
    "        (data['Name']==193)|\n",
    "        (data['Name']==685)|\n",
    "        (data['Name']==538)|\n",
    "        (data['Name']==406)|\n",
    "        (data['Name']==217)|\n",
    "        (data['Name']==648)|\n",
    "        (data['Name']==680)|\n",
    "        (data['Name']==798)|\n",
    "        (data['Name']==898)|\n",
    "        (data['Name']==677)|\n",
    "        (data['Name']==360)|\n",
    "        (data['Name']==163)|\n",
    "        (data['Name']==110)|\n",
    "        (data['Name']==386)|\n",
    "        (data['Name']==899)|\n",
    "        (data['Name']==991)|\n",
    "        (data['Name']==923)|\n",
    "        (data['Name']==692)|\n",
    "        (data['Name']==246)|\n",
    "        (data['Name']==647)|\n",
    "        (data['Name']==992)|\n",
    "        (data['Name']==172)|\n",
    "        (data['Name']==250)|\n",
    "        (data['Name']==894)|\n",
    "        (data['Name']==434)|\n",
    "        (data['Name']==182)|\n",
    "        (data['Name']==897)|\n",
    "        (data['Name']==993)|\n",
    "        (data['Name']==245)|\n",
    "        (data['Name']==751)|\n",
    "        (data['Name']==243)|\n",
    "        (data['Name']==997)|\n",
    "        (data['Name']==998)|\n",
    "        (data['Name']==988)|\n",
    "        (data['Name']==430)|\n",
    "        (data['Name']==435)|\n",
    "        (data['Name']==747)|\n",
    "        (data['Name']==305)|\n",
    "        (data['Name']==512)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "test1 = pd.DataFrame()\n",
    "train1 = pd.DataFrame()\n",
    "for index, row in train.iterrows():\n",
    "    count += 1\n",
    "    if (count%3)==0:\n",
    "        test1 = test1.append(row)\n",
    "    else:\n",
    "        train1 = train1.append(row)\n",
    "train1 = train\n",
    "test1 = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 673 entries, 0 to 1006\n",
      "Columns: 553 entries, Name to BCF\n",
      "dtypes: float64(375), int64(178)\n",
      "memory usage: 2.8 MB\n"
     ]
    }
   ],
   "source": [
    "train1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 334 entries, 2 to 1004\n",
      "Columns: 553 entries, Name to BCF\n",
      "dtypes: float64(375), int64(178)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "test1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = train1[[\"TopoPSA\", \"WPATH\", \"XLogP3\", \"BCF\"]]\n",
    "test1 = test1[[\"TopoPSA\", \"WPATH\", \"XLogP3\", \"BCF\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train1.drop(['BCF'], axis=1)\n",
    "y_train = train1['BCF']\n",
    "X_test = test1.drop(['BCF'], axis=1)\n",
    "y_test = test1['BCF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    532\n",
      "1    141\n",
      "Name: BCF, dtype: int64\n",
      "0    266\n",
      "1     68\n",
      "Name: BCF, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# just to see how it looks like..\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1. Imbalanced model towards nB compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbtree 0 0.7964071856287425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.80       334\n",
      "   macro avg       0.40      0.50      0.44       334\n",
      "weighted avg       0.63      0.80      0.71       334\n",
      "\n",
      "gbtree 1 0.8562874251497006\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91       266\n",
      "           1       0.63      0.71      0.67        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.78      0.80      0.79       334\n",
      "weighted avg       0.86      0.86      0.86       334\n",
      "\n",
      "gbtree 2 0.8562874251497006\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91       266\n",
      "           1       0.63      0.71      0.67        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.78      0.80      0.79       334\n",
      "weighted avg       0.86      0.86      0.86       334\n",
      "\n",
      "gbtree 3 0.8622754491017964\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       266\n",
      "           1       0.65      0.71      0.68        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.79      0.80      0.79       334\n",
      "weighted avg       0.87      0.86      0.86       334\n",
      "\n",
      "gbtree 4 0.8592814371257484\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       266\n",
      "           1       0.64      0.71      0.67        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.78      0.80      0.79       334\n",
      "weighted avg       0.87      0.86      0.86       334\n",
      "\n",
      "gbtree 5 0.8682634730538922\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       266\n",
      "           1       0.66      0.72      0.69        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.79      0.81      0.80       334\n",
      "weighted avg       0.87      0.87      0.87       334\n",
      "\n",
      "gbtree 6 0.8592814371257484\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91       266\n",
      "           1       0.63      0.74      0.68        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.78      0.81      0.80       334\n",
      "weighted avg       0.87      0.86      0.86       334\n",
      "\n",
      "gbtree 7 0.8592814371257484\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       266\n",
      "           1       0.64      0.71      0.67        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.78      0.80      0.79       334\n",
      "weighted avg       0.87      0.86      0.86       334\n",
      "\n",
      "gbtree 8 0.8622754491017964\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       266\n",
      "           1       0.65      0.71      0.68        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.79      0.80      0.79       334\n",
      "weighted avg       0.87      0.86      0.86       334\n",
      "\n",
      "gbtree 9 0.8622754491017964\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91       266\n",
      "           1       0.64      0.74      0.68        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.79      0.82      0.80       334\n",
      "weighted avg       0.87      0.86      0.87       334\n",
      "\n",
      "gbtree 10 0.8562874251497006\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91       266\n",
      "           1       0.63      0.71      0.67        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.78      0.80      0.79       334\n",
      "weighted avg       0.86      0.86      0.86       334\n",
      "\n",
      "gbtree 11 0.8592814371257484\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       266\n",
      "           1       0.64      0.71      0.67        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.78      0.80      0.79       334\n",
      "weighted avg       0.87      0.86      0.86       334\n",
      "\n",
      "gbtree 12 0.8712574850299402\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       266\n",
      "           1       0.67      0.74      0.70        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.80      0.82      0.81       334\n",
      "weighted avg       0.88      0.87      0.87       334\n",
      "\n",
      "gbtree 13 0.8622754491017964\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.91       266\n",
      "           1       0.64      0.72      0.68        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.79      0.81      0.80       334\n",
      "weighted avg       0.87      0.86      0.87       334\n",
      "\n",
      "gbtree 14 0.8622754491017964\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       266\n",
      "           1       0.65      0.71      0.68        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.79      0.80      0.79       334\n",
      "weighted avg       0.87      0.86      0.86       334\n",
      "\n",
      "gbtree 15 0.8622754491017964\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       266\n",
      "           1       0.65      0.71      0.68        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.79      0.80      0.79       334\n",
      "weighted avg       0.87      0.86      0.86       334\n",
      "\n",
      "gbtree 16 0.8682634730538922\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.92       266\n",
      "           1       0.66      0.74      0.69        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.79      0.82      0.81       334\n",
      "weighted avg       0.87      0.87      0.87       334\n",
      "\n",
      "gbtree 17 0.8652694610778443\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91       266\n",
      "           1       0.66      0.69      0.68        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.79      0.80      0.80       334\n",
      "weighted avg       0.87      0.87      0.87       334\n",
      "\n",
      "gbtree 18 0.8712574850299402\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92       266\n",
      "           1       0.68      0.71      0.69        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.80      0.81      0.80       334\n",
      "weighted avg       0.87      0.87      0.87       334\n",
      "\n",
      "gbtree 19 0.8622754491017964\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91       266\n",
      "           1       0.66      0.68      0.67        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.79      0.79      0.79       334\n",
      "weighted avg       0.86      0.86      0.86       334\n",
      "\n",
      "gbtree 20 0.8652694610778443\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91       266\n",
      "           1       0.66      0.71      0.68        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.79      0.81      0.80       334\n",
      "weighted avg       0.87      0.87      0.87       334\n",
      "\n",
      "gbtree 21 0.8712574850299402\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       266\n",
      "           1       0.67      0.72      0.70        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.80      0.82      0.81       334\n",
      "weighted avg       0.88      0.87      0.87       334\n",
      "\n",
      "gbtree 22 0.8712574850299402\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92       266\n",
      "           1       0.68      0.71      0.69        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.80      0.81      0.80       334\n",
      "weighted avg       0.87      0.87      0.87       334\n",
      "\n",
      "gbtree 23 0.8622754491017964\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91       266\n",
      "           1       0.65      0.69      0.67        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.79      0.80      0.79       334\n",
      "weighted avg       0.87      0.86      0.86       334\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbtree 24 0.8682634730538922\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92       266\n",
      "           1       0.67      0.69      0.68        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.80      0.80      0.80       334\n",
      "weighted avg       0.87      0.87      0.87       334\n",
      "\n",
      "gbtree 25 0.8712574850299402\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92       266\n",
      "           1       0.68      0.71      0.69        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.80      0.81      0.80       334\n",
      "weighted avg       0.87      0.87      0.87       334\n",
      "\n",
      "gbtree 26 0.874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       266\n",
      "           1       0.68      0.72      0.70        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.80      0.82      0.81       334\n",
      "weighted avg       0.88      0.87      0.88       334\n",
      "\n",
      "gbtree 27 0.8712574850299402\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92       266\n",
      "           1       0.68      0.71      0.69        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.80      0.81      0.80       334\n",
      "weighted avg       0.87      0.87      0.87       334\n",
      "\n",
      "gbtree 28 0.8652694610778443\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92       266\n",
      "           1       0.67      0.68      0.67        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.79      0.80      0.79       334\n",
      "weighted avg       0.87      0.87      0.87       334\n",
      "\n",
      "gbtree 29 0.8772455089820359\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92       266\n",
      "           1       0.70      0.69      0.70        68\n",
      "\n",
      "    accuracy                           0.88       334\n",
      "   macro avg       0.81      0.81      0.81       334\n",
      "weighted avg       0.88      0.88      0.88       334\n",
      "\n",
      "gblinear 0 0.7964071856287425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.80       334\n",
      "   macro avg       0.40      0.50      0.44       334\n",
      "weighted avg       0.63      0.80      0.71       334\n",
      "\n",
      "gblinear 1 0.7964071856287425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.80       334\n",
      "   macro avg       0.40      0.50      0.44       334\n",
      "weighted avg       0.63      0.80      0.71       334\n",
      "\n",
      "gblinear 2 0.7964071856287425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.80       334\n",
      "   macro avg       0.40      0.50      0.44       334\n",
      "weighted avg       0.63      0.80      0.71       334\n",
      "\n",
      "gblinear 3 0.7934131736526946\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.50      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 4 0.7934131736526946\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.50      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 5 0.7904191616766467\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.50      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 6 0.7874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.49      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 7 0.7874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.49      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 8 0.7874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.49      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 9 0.7874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.49      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 10 0.7874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.49      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 11 0.7874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.49      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 12 0.7874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.49      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 13 0.7874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.49      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 14 0.7874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.49      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 15 0.7874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.49      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 16 0.7874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.49      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 17 0.7874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.49      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 18 0.7874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.49      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 19 0.7874251497005988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.49      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 20 0.7874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.49      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 21 0.7874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.49      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 22 0.7874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.49      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 23 0.7874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.49      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 24 0.7874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.49      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 25 0.7874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.49      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 26 0.7874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.49      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 27 0.7874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.49      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 28 0.7874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.49      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "gblinear 29 0.7874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.79       334\n",
      "   macro avg       0.40      0.49      0.44       334\n",
      "weighted avg       0.63      0.79      0.70       334\n",
      "\n",
      "dart 0 0.7964071856287425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.80       334\n",
      "   macro avg       0.40      0.50      0.44       334\n",
      "weighted avg       0.63      0.80      0.71       334\n",
      "\n",
      "dart 1 0.8562874251497006\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91       266\n",
      "           1       0.63      0.71      0.67        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.78      0.80      0.79       334\n",
      "weighted avg       0.86      0.86      0.86       334\n",
      "\n",
      "dart 2 0.8562874251497006\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91       266\n",
      "           1       0.63      0.71      0.67        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.78      0.80      0.79       334\n",
      "weighted avg       0.86      0.86      0.86       334\n",
      "\n",
      "dart 3 0.8622754491017964\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       266\n",
      "           1       0.65      0.71      0.68        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.79      0.80      0.79       334\n",
      "weighted avg       0.87      0.86      0.86       334\n",
      "\n",
      "dart 4 0.8592814371257484\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       266\n",
      "           1       0.64      0.71      0.67        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.78      0.80      0.79       334\n",
      "weighted avg       0.87      0.86      0.86       334\n",
      "\n",
      "dart 5 0.8682634730538922\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       266\n",
      "           1       0.66      0.72      0.69        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.79      0.81      0.80       334\n",
      "weighted avg       0.87      0.87      0.87       334\n",
      "\n",
      "dart 6 0.8592814371257484\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91       266\n",
      "           1       0.63      0.74      0.68        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.78      0.81      0.80       334\n",
      "weighted avg       0.87      0.86      0.86       334\n",
      "\n",
      "dart 7 0.8592814371257484\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       266\n",
      "           1       0.64      0.71      0.67        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.78      0.80      0.79       334\n",
      "weighted avg       0.87      0.86      0.86       334\n",
      "\n",
      "dart 8 0.8622754491017964\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       266\n",
      "           1       0.65      0.71      0.68        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.79      0.80      0.79       334\n",
      "weighted avg       0.87      0.86      0.86       334\n",
      "\n",
      "dart 9 0.8622754491017964\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91       266\n",
      "           1       0.64      0.74      0.68        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.79      0.82      0.80       334\n",
      "weighted avg       0.87      0.86      0.87       334\n",
      "\n",
      "dart 10 0.8562874251497006\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91       266\n",
      "           1       0.63      0.71      0.67        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.78      0.80      0.79       334\n",
      "weighted avg       0.86      0.86      0.86       334\n",
      "\n",
      "dart 11 0.8592814371257484\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       266\n",
      "           1       0.64      0.71      0.67        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.78      0.80      0.79       334\n",
      "weighted avg       0.87      0.86      0.86       334\n",
      "\n",
      "dart 12 0.8712574850299402\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       266\n",
      "           1       0.67      0.74      0.70        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.80      0.82      0.81       334\n",
      "weighted avg       0.88      0.87      0.87       334\n",
      "\n",
      "dart 13 0.8622754491017964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.91       266\n",
      "           1       0.64      0.72      0.68        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.79      0.81      0.80       334\n",
      "weighted avg       0.87      0.86      0.87       334\n",
      "\n",
      "dart 14 0.8622754491017964\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       266\n",
      "           1       0.65      0.71      0.68        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.79      0.80      0.79       334\n",
      "weighted avg       0.87      0.86      0.86       334\n",
      "\n",
      "dart 15 0.8622754491017964\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       266\n",
      "           1       0.65      0.71      0.68        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.79      0.80      0.79       334\n",
      "weighted avg       0.87      0.86      0.86       334\n",
      "\n",
      "dart 16 0.8682634730538922\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.92       266\n",
      "           1       0.66      0.74      0.69        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.79      0.82      0.81       334\n",
      "weighted avg       0.87      0.87      0.87       334\n",
      "\n",
      "dart 17 0.8652694610778443\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91       266\n",
      "           1       0.66      0.69      0.68        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.79      0.80      0.80       334\n",
      "weighted avg       0.87      0.87      0.87       334\n",
      "\n",
      "dart 18 0.8712574850299402\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92       266\n",
      "           1       0.68      0.71      0.69        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.80      0.81      0.80       334\n",
      "weighted avg       0.87      0.87      0.87       334\n",
      "\n",
      "dart 19 0.8622754491017964\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91       266\n",
      "           1       0.66      0.68      0.67        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.79      0.79      0.79       334\n",
      "weighted avg       0.86      0.86      0.86       334\n",
      "\n",
      "dart 20 0.8652694610778443\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91       266\n",
      "           1       0.66      0.71      0.68        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.79      0.81      0.80       334\n",
      "weighted avg       0.87      0.87      0.87       334\n",
      "\n",
      "dart 21 0.8712574850299402\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       266\n",
      "           1       0.67      0.72      0.70        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.80      0.82      0.81       334\n",
      "weighted avg       0.88      0.87      0.87       334\n",
      "\n",
      "dart 22 0.8712574850299402\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92       266\n",
      "           1       0.68      0.71      0.69        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.80      0.81      0.80       334\n",
      "weighted avg       0.87      0.87      0.87       334\n",
      "\n",
      "dart 23 0.8622754491017964\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91       266\n",
      "           1       0.65      0.69      0.67        68\n",
      "\n",
      "    accuracy                           0.86       334\n",
      "   macro avg       0.79      0.80      0.79       334\n",
      "weighted avg       0.87      0.86      0.86       334\n",
      "\n",
      "dart 24 0.8682634730538922\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92       266\n",
      "           1       0.67      0.69      0.68        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.80      0.80      0.80       334\n",
      "weighted avg       0.87      0.87      0.87       334\n",
      "\n",
      "dart 25 0.8712574850299402\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92       266\n",
      "           1       0.68      0.71      0.69        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.80      0.81      0.80       334\n",
      "weighted avg       0.87      0.87      0.87       334\n",
      "\n",
      "dart 26 0.874251497005988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       266\n",
      "           1       0.68      0.72      0.70        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.80      0.82      0.81       334\n",
      "weighted avg       0.88      0.87      0.88       334\n",
      "\n",
      "dart 27 0.8712574850299402\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92       266\n",
      "           1       0.68      0.71      0.69        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.80      0.81      0.80       334\n",
      "weighted avg       0.87      0.87      0.87       334\n",
      "\n",
      "dart 28 0.8652694610778443\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92       266\n",
      "           1       0.67      0.68      0.67        68\n",
      "\n",
      "    accuracy                           0.87       334\n",
      "   macro avg       0.79      0.80      0.79       334\n",
      "weighted avg       0.87      0.87      0.87       334\n",
      "\n",
      "dart 29 0.8772455089820359\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92       266\n",
      "           1       0.70      0.69      0.70        68\n",
      "\n",
      "    accuracy                           0.88       334\n",
      "   macro avg       0.81      0.81      0.81       334\n",
      "weighted avg       0.88      0.88      0.88       334\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8772455089820359]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "boosters = ['gbtree', 'gblinear', 'dart']\n",
    "sample_types = ['uniform', 'weighted']\n",
    "\n",
    "for booster in boosters:\n",
    "    for i in range(0, 30, 1):\n",
    "        if booster == 'dart':\n",
    "            for sample_type in sample_types:\n",
    "                model=xgb.XGBClassifier(booster = booster, sample_type = sample_type, random_state=1, learning_rate=(i*0.01))\n",
    "        else:\n",
    "            model=xgb.XGBClassifier(booster = booster, random_state=1, learning_rate=(i*0.01))\n",
    "        model.fit(X_train, y_train)\n",
    "        scores.append([model.score(X_test,y_test)])\n",
    "        print(booster, i, model.score(X_test,y_test))\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(classification_report(y_test,y_pred))\n",
    "        \n",
    "max(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8772455089820359"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=xgb.XGBClassifier(boost = 'dart', random_state=1,learning_rate=0.29)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the new result\n",
    "M1_sp_XGBoost=0.91\n",
    "M1_se_XGBoost=0.66\n",
    "M1_acc_XGBoost=0.86\n",
    "# this is the allowed error:\n",
    "M1_err_sp=0.00\n",
    "M1_err_se=0.0194\n",
    "M1_err_acc=0.00\n",
    "# this is the reference result:\n",
    "M1_sp=0.91\n",
    "M1_se=0.7206\n",
    "M1_acc=0.87\n",
    "# this is the gain or loss: new result minus reference result\n",
    "M1_sp_XGBoost_eval=(M1_sp_XGBoost-M1_sp)\n",
    "M1_se_XGBoost_eval=(M1_se_XGBoost-M1_se)\n",
    "M1_acc_XGBoost_eval=(M1_acc_XGBoost-M1_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.1. Balanced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCF\n",
       "0    532\n",
       "1    141\n",
       "Name: BCF, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEFORE\n",
    "train1.groupby(['BCF'])['BCF'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCF\n",
       "0    141\n",
       "1    141\n",
       "Name: BCF, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_cases = train1[train1['BCF'] == 1]\n",
    "negative_sample = train1[train1['BCF'] == 0].sample(141, random_state=0)\n",
    "train1_balanced = pd.concat([positive_cases,negative_sample])\n",
    "train1_balanced.sort_index(inplace=True)\n",
    "\n",
    "X_train_bal = train1_balanced.drop(['BCF'], axis=1)\n",
    "y_train_bal = train1_balanced['BCF']\n",
    "\n",
    "# AFTER\n",
    "train1_balanced.groupby(['BCF'])['BCF'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbtree 0 0.7964071856287425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.80       334\n",
      "   macro avg       0.40      0.50      0.44       334\n",
      "weighted avg       0.63      0.80      0.71       334\n",
      "\n",
      "gbtree 1 0.7994011976047904\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.77      0.86       266\n",
      "           1       0.50      0.93      0.65        68\n",
      "\n",
      "    accuracy                           0.80       334\n",
      "   macro avg       0.74      0.85      0.76       334\n",
      "weighted avg       0.88      0.80      0.82       334\n",
      "\n",
      "gbtree 2 0.8233532934131736\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.80      0.88       266\n",
      "           1       0.54      0.90      0.67        68\n",
      "\n",
      "    accuracy                           0.82       334\n",
      "   macro avg       0.75      0.85      0.78       334\n",
      "weighted avg       0.88      0.82      0.84       334\n",
      "\n",
      "gbtree 3 0.8263473053892215\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.81      0.88       266\n",
      "           1       0.55      0.88      0.67        68\n",
      "\n",
      "    accuracy                           0.83       334\n",
      "   macro avg       0.75      0.85      0.78       334\n",
      "weighted avg       0.88      0.83      0.84       334\n",
      "\n",
      "gbtree 4 0.8383233532934131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "gbtree 5 0.8293413173652695\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.82      0.88       266\n",
      "           1       0.55      0.85      0.67        68\n",
      "\n",
      "    accuracy                           0.83       334\n",
      "   macro avg       0.75      0.84      0.78       334\n",
      "weighted avg       0.87      0.83      0.84       334\n",
      "\n",
      "gbtree 6 0.8383233532934131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.85      0.68        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.84      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "gbtree 7 0.8383233532934131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "gbtree 8 0.8413173652694611\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.77      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "gbtree 9 0.8383233532934131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "gbtree 10 0.8413173652694611\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.77      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "gbtree 11 0.844311377245509\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.84      0.90       266\n",
      "           1       0.58      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.77      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "gbtree 12 0.844311377245509\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.84      0.90       266\n",
      "           1       0.58      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.77      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "gbtree 13 0.844311377245509\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.84      0.90       266\n",
      "           1       0.58      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.77      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "gbtree 14 0.8383233532934131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "gbtree 15 0.8383233532934131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "gbtree 16 0.8413173652694611\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.77      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "gbtree 17 0.8353293413173652\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.82      0.89       266\n",
      "           1       0.56      0.88      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "gbtree 18 0.844311377245509\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.83      0.90       266\n",
      "           1       0.58      0.88      0.70        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.77      0.86      0.80       334\n",
      "weighted avg       0.89      0.84      0.85       334\n",
      "\n",
      "gbtree 19 0.8383233532934131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "gbtree 20 0.8413173652694611\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.83      0.89       266\n",
      "           1       0.57      0.88      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.77      0.86      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "gbtree 21 0.8383233532934131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "gbtree 22 0.8353293413173652\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.56      0.87      0.68        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "gbtree 23 0.8413173652694611\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.77      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbtree 24 0.8383233532934131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "gbtree 25 0.8353293413173652\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.56      0.87      0.68        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "gbtree 26 0.8413173652694611\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.77      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "gbtree 27 0.8293413173652695\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.82      0.88       266\n",
      "           1       0.55      0.85      0.67        68\n",
      "\n",
      "    accuracy                           0.83       334\n",
      "   macro avg       0.75      0.84      0.78       334\n",
      "weighted avg       0.87      0.83      0.84       334\n",
      "\n",
      "gbtree 28 0.8353293413173652\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.56      0.85      0.68        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.84      0.78       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "gbtree 29 0.8353293413173652\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.82      0.89       266\n",
      "           1       0.56      0.88      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "gblinear 0 0.7964071856287425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.80       334\n",
      "   macro avg       0.40      0.50      0.44       334\n",
      "weighted avg       0.63      0.80      0.71       334\n",
      "\n",
      "gblinear 1 0.688622754491018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.66      0.77       266\n",
      "           1       0.38      0.79      0.51        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.65      0.73      0.64       334\n",
      "weighted avg       0.81      0.69      0.72       334\n",
      "\n",
      "gblinear 2 0.7125748502994012\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.69      0.79       266\n",
      "           1       0.40      0.79      0.53        68\n",
      "\n",
      "    accuracy                           0.71       334\n",
      "   macro avg       0.66      0.74      0.66       334\n",
      "weighted avg       0.82      0.71      0.74       334\n",
      "\n",
      "gblinear 3 0.7275449101796407\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.71      0.81       266\n",
      "           1       0.41      0.79      0.54        68\n",
      "\n",
      "    accuracy                           0.73       334\n",
      "   macro avg       0.67      0.75      0.67       334\n",
      "weighted avg       0.83      0.73      0.75       334\n",
      "\n",
      "gblinear 4 0.7365269461077845\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.72      0.81       266\n",
      "           1       0.42      0.79      0.55        68\n",
      "\n",
      "    accuracy                           0.74       334\n",
      "   macro avg       0.68      0.76      0.68       334\n",
      "weighted avg       0.83      0.74      0.76       334\n",
      "\n",
      "gblinear 5 0.7395209580838323\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.73      0.82       266\n",
      "           1       0.43      0.79      0.55        68\n",
      "\n",
      "    accuracy                           0.74       334\n",
      "   macro avg       0.68      0.76      0.68       334\n",
      "weighted avg       0.83      0.74      0.76       334\n",
      "\n",
      "gblinear 6 0.7395209580838323\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.73      0.82       266\n",
      "           1       0.43      0.79      0.55        68\n",
      "\n",
      "    accuracy                           0.74       334\n",
      "   macro avg       0.68      0.76      0.68       334\n",
      "weighted avg       0.83      0.74      0.76       334\n",
      "\n",
      "gblinear 7 0.7455089820359282\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.73      0.82       266\n",
      "           1       0.43      0.79      0.56        68\n",
      "\n",
      "    accuracy                           0.75       334\n",
      "   macro avg       0.68      0.76      0.69       334\n",
      "weighted avg       0.83      0.75      0.77       334\n",
      "\n",
      "gblinear 8 0.7514970059880239\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.74      0.83       266\n",
      "           1       0.44      0.79      0.57        68\n",
      "\n",
      "    accuracy                           0.75       334\n",
      "   macro avg       0.69      0.77      0.70       334\n",
      "weighted avg       0.83      0.75      0.77       334\n",
      "\n",
      "gblinear 9 0.7574850299401198\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.75      0.83       266\n",
      "           1       0.45      0.79      0.57        68\n",
      "\n",
      "    accuracy                           0.76       334\n",
      "   macro avg       0.69      0.77      0.70       334\n",
      "weighted avg       0.83      0.76      0.78       334\n",
      "\n",
      "gblinear 10 0.7604790419161677\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.75      0.83       266\n",
      "           1       0.45      0.79      0.57        68\n",
      "\n",
      "    accuracy                           0.76       334\n",
      "   macro avg       0.69      0.77      0.70       334\n",
      "weighted avg       0.84      0.76      0.78       334\n",
      "\n",
      "gblinear 11 0.7604790419161677\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.75      0.83       266\n",
      "           1       0.45      0.79      0.57        68\n",
      "\n",
      "    accuracy                           0.76       334\n",
      "   macro avg       0.69      0.77      0.70       334\n",
      "weighted avg       0.84      0.76      0.78       334\n",
      "\n",
      "gblinear 12 0.7604790419161677\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.75      0.83       266\n",
      "           1       0.45      0.79      0.57        68\n",
      "\n",
      "    accuracy                           0.76       334\n",
      "   macro avg       0.69      0.77      0.70       334\n",
      "weighted avg       0.84      0.76      0.78       334\n",
      "\n",
      "gblinear 13 0.7604790419161677\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.75      0.83       266\n",
      "           1       0.45      0.79      0.57        68\n",
      "\n",
      "    accuracy                           0.76       334\n",
      "   macro avg       0.69      0.77      0.70       334\n",
      "weighted avg       0.84      0.76      0.78       334\n",
      "\n",
      "gblinear 14 0.7604790419161677\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.75      0.83       266\n",
      "           1       0.45      0.79      0.57        68\n",
      "\n",
      "    accuracy                           0.76       334\n",
      "   macro avg       0.69      0.77      0.70       334\n",
      "weighted avg       0.84      0.76      0.78       334\n",
      "\n",
      "gblinear 15 0.7604790419161677\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.75      0.83       266\n",
      "           1       0.45      0.79      0.57        68\n",
      "\n",
      "    accuracy                           0.76       334\n",
      "   macro avg       0.69      0.77      0.70       334\n",
      "weighted avg       0.84      0.76      0.78       334\n",
      "\n",
      "gblinear 16 0.7604790419161677\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.75      0.83       266\n",
      "           1       0.45      0.79      0.57        68\n",
      "\n",
      "    accuracy                           0.76       334\n",
      "   macro avg       0.69      0.77      0.70       334\n",
      "weighted avg       0.84      0.76      0.78       334\n",
      "\n",
      "gblinear 17 0.7634730538922155\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.76      0.84       266\n",
      "           1       0.45      0.79      0.58        68\n",
      "\n",
      "    accuracy                           0.76       334\n",
      "   macro avg       0.69      0.77      0.71       334\n",
      "weighted avg       0.84      0.76      0.78       334\n",
      "\n",
      "gblinear 18 0.7664670658682635\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.76      0.84       266\n",
      "           1       0.46      0.79      0.58        68\n",
      "\n",
      "    accuracy                           0.77       334\n",
      "   macro avg       0.70      0.78      0.71       334\n",
      "weighted avg       0.84      0.77      0.79       334\n",
      "\n",
      "gblinear 19 0.7664670658682635\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.76      0.84       266\n",
      "           1       0.46      0.79      0.58        68\n",
      "\n",
      "    accuracy                           0.77       334\n",
      "   macro avg       0.70      0.78      0.71       334\n",
      "weighted avg       0.84      0.77      0.79       334\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gblinear 20 0.7664670658682635\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.76      0.84       266\n",
      "           1       0.46      0.79      0.58        68\n",
      "\n",
      "    accuracy                           0.77       334\n",
      "   macro avg       0.70      0.78      0.71       334\n",
      "weighted avg       0.84      0.77      0.79       334\n",
      "\n",
      "gblinear 21 0.7664670658682635\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.76      0.84       266\n",
      "           1       0.46      0.79      0.58        68\n",
      "\n",
      "    accuracy                           0.77       334\n",
      "   macro avg       0.70      0.78      0.71       334\n",
      "weighted avg       0.84      0.77      0.79       334\n",
      "\n",
      "gblinear 22 0.7664670658682635\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.76      0.84       266\n",
      "           1       0.46      0.79      0.58        68\n",
      "\n",
      "    accuracy                           0.77       334\n",
      "   macro avg       0.70      0.78      0.71       334\n",
      "weighted avg       0.84      0.77      0.79       334\n",
      "\n",
      "gblinear 23 0.7664670658682635\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.76      0.84       266\n",
      "           1       0.46      0.79      0.58        68\n",
      "\n",
      "    accuracy                           0.77       334\n",
      "   macro avg       0.70      0.78      0.71       334\n",
      "weighted avg       0.84      0.77      0.79       334\n",
      "\n",
      "gblinear 24 0.7664670658682635\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.76      0.84       266\n",
      "           1       0.46      0.79      0.58        68\n",
      "\n",
      "    accuracy                           0.77       334\n",
      "   macro avg       0.70      0.78      0.71       334\n",
      "weighted avg       0.84      0.77      0.79       334\n",
      "\n",
      "gblinear 25 0.7694610778443114\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.76      0.84       266\n",
      "           1       0.46      0.79      0.58        68\n",
      "\n",
      "    accuracy                           0.77       334\n",
      "   macro avg       0.70      0.78      0.71       334\n",
      "weighted avg       0.84      0.77      0.79       334\n",
      "\n",
      "gblinear 26 0.7694610778443114\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.76      0.84       266\n",
      "           1       0.46      0.79      0.58        68\n",
      "\n",
      "    accuracy                           0.77       334\n",
      "   macro avg       0.70      0.78      0.71       334\n",
      "weighted avg       0.84      0.77      0.79       334\n",
      "\n",
      "gblinear 27 0.7694610778443114\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.76      0.84       266\n",
      "           1       0.46      0.79      0.58        68\n",
      "\n",
      "    accuracy                           0.77       334\n",
      "   macro avg       0.70      0.78      0.71       334\n",
      "weighted avg       0.84      0.77      0.79       334\n",
      "\n",
      "gblinear 28 0.7694610778443114\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.76      0.84       266\n",
      "           1       0.46      0.79      0.58        68\n",
      "\n",
      "    accuracy                           0.77       334\n",
      "   macro avg       0.70      0.78      0.71       334\n",
      "weighted avg       0.84      0.77      0.79       334\n",
      "\n",
      "gblinear 29 0.7694610778443114\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.76      0.84       266\n",
      "           1       0.46      0.79      0.58        68\n",
      "\n",
      "    accuracy                           0.77       334\n",
      "   macro avg       0.70      0.78      0.71       334\n",
      "weighted avg       0.84      0.77      0.79       334\n",
      "\n",
      "dart 0 0.7964071856287425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.80       334\n",
      "   macro avg       0.40      0.50      0.44       334\n",
      "weighted avg       0.63      0.80      0.71       334\n",
      "\n",
      "dart 1 0.7994011976047904\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.77      0.86       266\n",
      "           1       0.50      0.93      0.65        68\n",
      "\n",
      "    accuracy                           0.80       334\n",
      "   macro avg       0.74      0.85      0.76       334\n",
      "weighted avg       0.88      0.80      0.82       334\n",
      "\n",
      "dart 2 0.8233532934131736\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.80      0.88       266\n",
      "           1       0.54      0.90      0.67        68\n",
      "\n",
      "    accuracy                           0.82       334\n",
      "   macro avg       0.75      0.85      0.78       334\n",
      "weighted avg       0.88      0.82      0.84       334\n",
      "\n",
      "dart 3 0.8263473053892215\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.81      0.88       266\n",
      "           1       0.55      0.88      0.67        68\n",
      "\n",
      "    accuracy                           0.83       334\n",
      "   macro avg       0.75      0.85      0.78       334\n",
      "weighted avg       0.88      0.83      0.84       334\n",
      "\n",
      "dart 4 0.8383233532934131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "dart 5 0.8293413173652695\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.82      0.88       266\n",
      "           1       0.55      0.85      0.67        68\n",
      "\n",
      "    accuracy                           0.83       334\n",
      "   macro avg       0.75      0.84      0.78       334\n",
      "weighted avg       0.87      0.83      0.84       334\n",
      "\n",
      "dart 6 0.8383233532934131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.85      0.68        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.84      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "dart 7 0.8383233532934131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "dart 8 0.8413173652694611\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.77      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "dart 9 0.8383233532934131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "dart 10 0.8413173652694611\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.77      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "dart 11 0.844311377245509\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.84      0.90       266\n",
      "           1       0.58      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.77      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "dart 12 0.844311377245509\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.84      0.90       266\n",
      "           1       0.58      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.77      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "dart 13 0.844311377245509\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.84      0.90       266\n",
      "           1       0.58      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.77      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dart 14 0.8383233532934131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "dart 15 0.8383233532934131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "dart 16 0.8413173652694611\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.77      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "dart 17 0.8353293413173652\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.82      0.89       266\n",
      "           1       0.56      0.88      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "dart 18 0.844311377245509\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.83      0.90       266\n",
      "           1       0.58      0.88      0.70        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.77      0.86      0.80       334\n",
      "weighted avg       0.89      0.84      0.85       334\n",
      "\n",
      "dart 19 0.8383233532934131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "dart 20 0.8413173652694611\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.83      0.89       266\n",
      "           1       0.57      0.88      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.77      0.86      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "dart 21 0.8383233532934131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "dart 22 0.8353293413173652\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.56      0.87      0.68        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "dart 23 0.8413173652694611\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.77      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "dart 24 0.8383233532934131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "dart 25 0.8353293413173652\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.56      0.87      0.68        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "dart 26 0.8413173652694611\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.57      0.87      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.77      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "dart 27 0.8293413173652695\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.82      0.88       266\n",
      "           1       0.55      0.85      0.67        68\n",
      "\n",
      "    accuracy                           0.83       334\n",
      "   macro avg       0.75      0.84      0.78       334\n",
      "weighted avg       0.87      0.83      0.84       334\n",
      "\n",
      "dart 28 0.8353293413173652\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       266\n",
      "           1       0.56      0.85      0.68        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.84      0.78       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n",
      "dart 29 0.8353293413173652\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.82      0.89       266\n",
      "           1       0.56      0.88      0.69        68\n",
      "\n",
      "    accuracy                           0.84       334\n",
      "   macro avg       0.76      0.85      0.79       334\n",
      "weighted avg       0.88      0.84      0.85       334\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.844311377245509]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_bal = []\n",
    "boosters = ['gbtree', 'gblinear', 'dart']\n",
    "for booster in boosters:\n",
    "    for i in range(0, 30, 1):\n",
    "        model=xgb.XGBClassifier(booster = booster, random_state=1, learning_rate=(i*0.01))\n",
    "        model.fit(X_train_bal, y_train_bal)\n",
    "        scores_bal.append([model.score(X_test,y_test)])\n",
    "        print(booster, i, model.score(X_test,y_test))\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(classification_report(y_test,y_pred))\n",
    "max(scores_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.11, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=1,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model=xgb.XGBClassifier(booster = 'gbtree', random_state=1,learning_rate=0.11)\n",
    "model.fit(X_train_bal, y_train_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the new result\n",
    "M2_sp_XGBoost=0.83\n",
    "M2_se_XGBoost=0.74\n",
    "M2_acc_XGBoost=0.81\n",
    "# this is the allowed error:\n",
    "M2_err_sp=0.00\n",
    "M2_err_se=0.0129\n",
    "M2_err_acc=0.01\n",
    "# this is the reference result:\n",
    "M2_sp=0.83\n",
    "M2_se=0.91\n",
    "M2_acc=0.85\n",
    "# this is the gain or loss: new result minus reference result\n",
    "M2_sp_XGBoost_eval=(M2_sp_XGBoost-M2_sp)\n",
    "M2_se_XGBoost_eval=(M2_se_XGBoost-M2_se)\n",
    "M2_acc_XGBoost_eval=(M2_acc_XGBoost-M2_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.2 Balanced Model-SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=0)\n",
    "X_train_resampled, y_train_resampled = smote.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.874251497005988]\n"
     ]
    }
   ],
   "source": [
    "scores_resampled = []\n",
    "boosters = ['gbtree', 'gblinear', 'dart']\n",
    "for booster in boosters:\n",
    "    for i in range(0, 30, 1):\n",
    "        model=xgb.XGBClassifier(booster = booster, random_state=1, learning_rate=(i*0.01))\n",
    "        model.fit(X_train_resampled, y_train_resampled)\n",
    "        scores_resampled.append([model.score(X_test,y_test)])\n",
    "        print(booster, i, model.score(X_test,y_test))\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(classification_report(y_test,y_pred))\n",
    "max(scores_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=xgb.XGBClassifier(booster='dart', random_state=1,learning_rate=0.25)\n",
    "model.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the new result\n",
    "M2_2_sp_XGBoost=0.85\n",
    "M2_2_se_XGBoost=0.75\n",
    "M2_2_acc_XGBoost=0.83\n",
    "# this is the allowed error:\n",
    "M2_2_err_sp=0.06\n",
    "M2_2_err_se=0.0571\n",
    "M2_2_err_acc=0.04\n",
    "# this is the reference result:\n",
    "M2_2_sp=0.89\n",
    "M2_2_se=0.84\n",
    "M2_2_acc=0.88\n",
    "# this is the gain or loss: new result minus reference result\n",
    "M2_2_sp_XGBoost_eval=(M2_2_sp_XGBoost-M2_2_sp)\n",
    "M2_2_se_XGBoost_eval=(M2_2_se_XGBoost-M2_2_se)\n",
    "M2_2_acc_XGBoost_eval=(M2_2_acc_XGBoost-M2_2_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3. Imbalanced model towards B compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCF\n",
       "0    532\n",
       "1    141\n",
       "Name: BCF, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEFORE\n",
    "train1.groupby(['BCF'])['BCF'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCF\n",
       "0     28\n",
       "1    107\n",
       "Name: BCF, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_cases = train1[train1['BCF'] == 1].sample(107, random_state=0)\n",
    "negative_sample = train1[train1['BCF'] == 0].sample(28, random_state=0)\n",
    "train1_imbalanced = pd.concat([positive_cases,negative_sample])\n",
    "train1_imbalanced.sort_index(inplace=True)\n",
    "\n",
    "X_train_imbal = train1_imbalanced.drop(['BCF'], axis=1)\n",
    "y_train_imbal = train1_imbalanced['BCF']\n",
    "# AFTER\n",
    "train1_imbalanced.groupby(['BCF'])['BCF'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbtree 0 0.7964071856287425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.80       334\n",
      "   macro avg       0.40      0.50      0.44       334\n",
      "weighted avg       0.63      0.80      0.71       334\n",
      "\n",
      "gbtree 1 0.688622754491018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.62      0.76       266\n",
      "           1       0.39      0.97      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.79      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "gbtree 2 0.6796407185628742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75       266\n",
      "           1       0.39      1.00      0.56        68\n",
      "\n",
      "    accuracy                           0.68       334\n",
      "   macro avg       0.69      0.80      0.65       334\n",
      "weighted avg       0.88      0.68      0.71       334\n",
      "\n",
      "gbtree 3 0.6796407185628742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75       266\n",
      "           1       0.39      1.00      0.56        68\n",
      "\n",
      "    accuracy                           0.68       334\n",
      "   macro avg       0.69      0.80      0.65       334\n",
      "weighted avg       0.88      0.68      0.71       334\n",
      "\n",
      "gbtree 4 0.6796407185628742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75       266\n",
      "           1       0.39      1.00      0.56        68\n",
      "\n",
      "    accuracy                           0.68       334\n",
      "   macro avg       0.69      0.80      0.65       334\n",
      "weighted avg       0.88      0.68      0.71       334\n",
      "\n",
      "gbtree 5 0.6796407185628742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75       266\n",
      "           1       0.39      1.00      0.56        68\n",
      "\n",
      "    accuracy                           0.68       334\n",
      "   macro avg       0.69      0.80      0.65       334\n",
      "weighted avg       0.88      0.68      0.71       334\n",
      "\n",
      "gbtree 6 0.6796407185628742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75       266\n",
      "           1       0.39      1.00      0.56        68\n",
      "\n",
      "    accuracy                           0.68       334\n",
      "   macro avg       0.69      0.80      0.65       334\n",
      "weighted avg       0.88      0.68      0.71       334\n",
      "\n",
      "gbtree 7 0.6826347305389222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75       266\n",
      "           1       0.39      1.00      0.56        68\n",
      "\n",
      "    accuracy                           0.68       334\n",
      "   macro avg       0.70      0.80      0.66       334\n",
      "weighted avg       0.88      0.68      0.71       334\n",
      "\n",
      "gbtree 8 0.6826347305389222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75       266\n",
      "           1       0.39      1.00      0.56        68\n",
      "\n",
      "    accuracy                           0.68       334\n",
      "   macro avg       0.70      0.80      0.66       334\n",
      "weighted avg       0.88      0.68      0.71       334\n",
      "\n",
      "gbtree 9 0.6856287425149701\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.61      0.75       266\n",
      "           1       0.39      1.00      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.70      0.80      0.66       334\n",
      "weighted avg       0.88      0.69      0.72       334\n",
      "\n",
      "gbtree 10 0.6826347305389222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75       266\n",
      "           1       0.39      1.00      0.56        68\n",
      "\n",
      "    accuracy                           0.68       334\n",
      "   macro avg       0.70      0.80      0.66       334\n",
      "weighted avg       0.88      0.68      0.71       334\n",
      "\n",
      "gbtree 11 0.688622754491018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.61      0.76       266\n",
      "           1       0.40      1.00      0.57        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.70      0.80      0.66       334\n",
      "weighted avg       0.88      0.69      0.72       334\n",
      "\n",
      "gbtree 12 0.6826347305389222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75       266\n",
      "           1       0.39      1.00      0.56        68\n",
      "\n",
      "    accuracy                           0.68       334\n",
      "   macro avg       0.70      0.80      0.66       334\n",
      "weighted avg       0.88      0.68      0.71       334\n",
      "\n",
      "gbtree 13 0.6826347305389222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.75       266\n",
      "           1       0.39      0.99      0.56        68\n",
      "\n",
      "    accuracy                           0.68       334\n",
      "   macro avg       0.69      0.80      0.66       334\n",
      "weighted avg       0.87      0.68      0.71       334\n",
      "\n",
      "gbtree 14 0.6856287425149701\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76       266\n",
      "           1       0.39      0.97      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.79      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "gbtree 15 0.688622754491018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.62      0.76       266\n",
      "           1       0.39      0.97      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.79      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "gbtree 16 0.688622754491018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76       266\n",
      "           1       0.39      0.99      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.80      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "gbtree 17 0.688622754491018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76       266\n",
      "           1       0.39      0.99      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.80      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "gbtree 18 0.688622754491018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76       266\n",
      "           1       0.39      0.99      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.80      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "gbtree 19 0.6856287425149701\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76       266\n",
      "           1       0.39      0.97      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.79      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "gbtree 20 0.688622754491018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.62      0.76       266\n",
      "           1       0.39      0.96      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.79      0.66       334\n",
      "weighted avg       0.86      0.69      0.72       334\n",
      "\n",
      "gbtree 21 0.6856287425149701\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76       266\n",
      "           1       0.39      0.97      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.79      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "gbtree 22 0.6946107784431138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.62      0.76       266\n",
      "           1       0.40      0.99      0.57        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.70      0.80      0.67       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "gbtree 23 0.688622754491018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76       266\n",
      "           1       0.39      0.99      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.80      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbtree 24 0.6916167664670658\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.61      0.76       266\n",
      "           1       0.40      1.00      0.57        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.70      0.81      0.66       334\n",
      "weighted avg       0.88      0.69      0.72       334\n",
      "\n",
      "gbtree 25 0.688622754491018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76       266\n",
      "           1       0.39      0.99      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.80      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "gbtree 26 0.6856287425149701\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76       266\n",
      "           1       0.39      0.99      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.80      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "gbtree 27 0.6916167664670658\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.62      0.76       266\n",
      "           1       0.40      0.97      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.80      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "gbtree 28 0.688622754491018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76       266\n",
      "           1       0.39      0.99      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.80      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "gbtree 29 0.688622754491018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76       266\n",
      "           1       0.39      0.99      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.80      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "gblinear 0 0.7964071856287425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.80       334\n",
      "   macro avg       0.40      0.50      0.44       334\n",
      "weighted avg       0.63      0.80      0.71       334\n",
      "\n",
      "gblinear 1 0.2125748502994012\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.02       266\n",
      "           1       0.21      1.00      0.34        68\n",
      "\n",
      "    accuracy                           0.21       334\n",
      "   macro avg       0.60      0.51      0.18       334\n",
      "weighted avg       0.84      0.21      0.09       334\n",
      "\n",
      "gblinear 2 0.25748502994011974\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.07      0.13       266\n",
      "           1       0.22      1.00      0.35        68\n",
      "\n",
      "    accuracy                           0.26       334\n",
      "   macro avg       0.61      0.53      0.24       334\n",
      "weighted avg       0.84      0.26      0.17       334\n",
      "\n",
      "gblinear 3 0.281437125748503\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.10      0.18       266\n",
      "           1       0.22      1.00      0.36        68\n",
      "\n",
      "    accuracy                           0.28       334\n",
      "   macro avg       0.61      0.55      0.27       334\n",
      "weighted avg       0.84      0.28      0.22       334\n",
      "\n",
      "gblinear 4 0.2994011976047904\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.12      0.21       266\n",
      "           1       0.23      1.00      0.37        68\n",
      "\n",
      "    accuracy                           0.30       334\n",
      "   macro avg       0.61      0.56      0.29       334\n",
      "weighted avg       0.84      0.30      0.25       334\n",
      "\n",
      "gblinear 5 0.3143712574850299\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.14      0.24       266\n",
      "           1       0.23      1.00      0.37        68\n",
      "\n",
      "    accuracy                           0.31       334\n",
      "   macro avg       0.61      0.57      0.31       334\n",
      "weighted avg       0.84      0.31      0.27       334\n",
      "\n",
      "gblinear 6 0.32335329341317365\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.15      0.26       266\n",
      "           1       0.23      1.00      0.38        68\n",
      "\n",
      "    accuracy                           0.32       334\n",
      "   macro avg       0.62      0.58      0.32       334\n",
      "weighted avg       0.84      0.32      0.28       334\n",
      "\n",
      "gblinear 7 0.3263473053892216\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.15      0.27       266\n",
      "           1       0.23      1.00      0.38        68\n",
      "\n",
      "    accuracy                           0.33       334\n",
      "   macro avg       0.62      0.58      0.32       334\n",
      "weighted avg       0.84      0.33      0.29       334\n",
      "\n",
      "gblinear 8 0.32934131736526945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.27       266\n",
      "           1       0.23      1.00      0.38        68\n",
      "\n",
      "    accuracy                           0.33       334\n",
      "   macro avg       0.62      0.58      0.33       334\n",
      "weighted avg       0.84      0.33      0.29       334\n",
      "\n",
      "gblinear 9 0.32934131736526945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.27       266\n",
      "           1       0.23      1.00      0.38        68\n",
      "\n",
      "    accuracy                           0.33       334\n",
      "   macro avg       0.62      0.58      0.33       334\n",
      "weighted avg       0.84      0.33      0.29       334\n",
      "\n",
      "gblinear 10 0.32934131736526945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.27       266\n",
      "           1       0.23      1.00      0.38        68\n",
      "\n",
      "    accuracy                           0.33       334\n",
      "   macro avg       0.62      0.58      0.33       334\n",
      "weighted avg       0.84      0.33      0.29       334\n",
      "\n",
      "gblinear 11 0.32934131736526945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.27       266\n",
      "           1       0.23      1.00      0.38        68\n",
      "\n",
      "    accuracy                           0.33       334\n",
      "   macro avg       0.62      0.58      0.33       334\n",
      "weighted avg       0.84      0.33      0.29       334\n",
      "\n",
      "gblinear 12 0.32934131736526945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.27       266\n",
      "           1       0.23      1.00      0.38        68\n",
      "\n",
      "    accuracy                           0.33       334\n",
      "   macro avg       0.62      0.58      0.33       334\n",
      "weighted avg       0.84      0.33      0.29       334\n",
      "\n",
      "gblinear 13 0.32934131736526945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.27       266\n",
      "           1       0.23      1.00      0.38        68\n",
      "\n",
      "    accuracy                           0.33       334\n",
      "   macro avg       0.62      0.58      0.33       334\n",
      "weighted avg       0.84      0.33      0.29       334\n",
      "\n",
      "gblinear 14 0.32934131736526945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.27       266\n",
      "           1       0.23      1.00      0.38        68\n",
      "\n",
      "    accuracy                           0.33       334\n",
      "   macro avg       0.62      0.58      0.33       334\n",
      "weighted avg       0.84      0.33      0.29       334\n",
      "\n",
      "gblinear 15 0.32934131736526945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.27       266\n",
      "           1       0.23      1.00      0.38        68\n",
      "\n",
      "    accuracy                           0.33       334\n",
      "   macro avg       0.62      0.58      0.33       334\n",
      "weighted avg       0.84      0.33      0.29       334\n",
      "\n",
      "gblinear 16 0.32934131736526945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.27       266\n",
      "           1       0.23      1.00      0.38        68\n",
      "\n",
      "    accuracy                           0.33       334\n",
      "   macro avg       0.62      0.58      0.33       334\n",
      "weighted avg       0.84      0.33      0.29       334\n",
      "\n",
      "gblinear 17 0.32934131736526945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.27       266\n",
      "           1       0.23      1.00      0.38        68\n",
      "\n",
      "    accuracy                           0.33       334\n",
      "   macro avg       0.62      0.58      0.33       334\n",
      "weighted avg       0.84      0.33      0.29       334\n",
      "\n",
      "gblinear 18 0.32934131736526945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.27       266\n",
      "           1       0.23      1.00      0.38        68\n",
      "\n",
      "    accuracy                           0.33       334\n",
      "   macro avg       0.62      0.58      0.33       334\n",
      "weighted avg       0.84      0.33      0.29       334\n",
      "\n",
      "gblinear 19 0.32934131736526945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.27       266\n",
      "           1       0.23      1.00      0.38        68\n",
      "\n",
      "    accuracy                           0.33       334\n",
      "   macro avg       0.62      0.58      0.33       334\n",
      "weighted avg       0.84      0.33      0.29       334\n",
      "\n",
      "gblinear 20 0.32934131736526945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.27       266\n",
      "           1       0.23      1.00      0.38        68\n",
      "\n",
      "    accuracy                           0.33       334\n",
      "   macro avg       0.62      0.58      0.33       334\n",
      "weighted avg       0.84      0.33      0.29       334\n",
      "\n",
      "gblinear 21 0.32934131736526945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.27       266\n",
      "           1       0.23      1.00      0.38        68\n",
      "\n",
      "    accuracy                           0.33       334\n",
      "   macro avg       0.62      0.58      0.33       334\n",
      "weighted avg       0.84      0.33      0.29       334\n",
      "\n",
      "gblinear 22 0.32934131736526945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.27       266\n",
      "           1       0.23      1.00      0.38        68\n",
      "\n",
      "    accuracy                           0.33       334\n",
      "   macro avg       0.62      0.58      0.33       334\n",
      "weighted avg       0.84      0.33      0.29       334\n",
      "\n",
      "gblinear 23 0.32934131736526945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.27       266\n",
      "           1       0.23      1.00      0.38        68\n",
      "\n",
      "    accuracy                           0.33       334\n",
      "   macro avg       0.62      0.58      0.33       334\n",
      "weighted avg       0.84      0.33      0.29       334\n",
      "\n",
      "gblinear 24 0.32934131736526945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.27       266\n",
      "           1       0.23      1.00      0.38        68\n",
      "\n",
      "    accuracy                           0.33       334\n",
      "   macro avg       0.62      0.58      0.33       334\n",
      "weighted avg       0.84      0.33      0.29       334\n",
      "\n",
      "gblinear 25 0.32934131736526945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.27       266\n",
      "           1       0.23      1.00      0.38        68\n",
      "\n",
      "    accuracy                           0.33       334\n",
      "   macro avg       0.62      0.58      0.33       334\n",
      "weighted avg       0.84      0.33      0.29       334\n",
      "\n",
      "gblinear 26 0.32934131736526945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.27       266\n",
      "           1       0.23      1.00      0.38        68\n",
      "\n",
      "    accuracy                           0.33       334\n",
      "   macro avg       0.62      0.58      0.33       334\n",
      "weighted avg       0.84      0.33      0.29       334\n",
      "\n",
      "gblinear 27 0.3323353293413174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.28       266\n",
      "           1       0.23      1.00      0.38        68\n",
      "\n",
      "    accuracy                           0.33       334\n",
      "   macro avg       0.62      0.58      0.33       334\n",
      "weighted avg       0.84      0.33      0.30       334\n",
      "\n",
      "gblinear 28 0.3323353293413174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.28       266\n",
      "           1       0.23      1.00      0.38        68\n",
      "\n",
      "    accuracy                           0.33       334\n",
      "   macro avg       0.62      0.58      0.33       334\n",
      "weighted avg       0.84      0.33      0.30       334\n",
      "\n",
      "gblinear 29 0.3323353293413174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.28       266\n",
      "           1       0.23      1.00      0.38        68\n",
      "\n",
      "    accuracy                           0.33       334\n",
      "   macro avg       0.62      0.58      0.33       334\n",
      "weighted avg       0.84      0.33      0.30       334\n",
      "\n",
      "dart 0 0.7964071856287425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89       266\n",
      "           1       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.80       334\n",
      "   macro avg       0.40      0.50      0.44       334\n",
      "weighted avg       0.63      0.80      0.71       334\n",
      "\n",
      "dart 1 0.688622754491018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.62      0.76       266\n",
      "           1       0.39      0.97      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.79      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "dart 2 0.6796407185628742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75       266\n",
      "           1       0.39      1.00      0.56        68\n",
      "\n",
      "    accuracy                           0.68       334\n",
      "   macro avg       0.69      0.80      0.65       334\n",
      "weighted avg       0.88      0.68      0.71       334\n",
      "\n",
      "dart 3 0.6796407185628742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75       266\n",
      "           1       0.39      1.00      0.56        68\n",
      "\n",
      "    accuracy                           0.68       334\n",
      "   macro avg       0.69      0.80      0.65       334\n",
      "weighted avg       0.88      0.68      0.71       334\n",
      "\n",
      "dart 4 0.6796407185628742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75       266\n",
      "           1       0.39      1.00      0.56        68\n",
      "\n",
      "    accuracy                           0.68       334\n",
      "   macro avg       0.69      0.80      0.65       334\n",
      "weighted avg       0.88      0.68      0.71       334\n",
      "\n",
      "dart 5 0.6796407185628742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75       266\n",
      "           1       0.39      1.00      0.56        68\n",
      "\n",
      "    accuracy                           0.68       334\n",
      "   macro avg       0.69      0.80      0.65       334\n",
      "weighted avg       0.88      0.68      0.71       334\n",
      "\n",
      "dart 6 0.6796407185628742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75       266\n",
      "           1       0.39      1.00      0.56        68\n",
      "\n",
      "    accuracy                           0.68       334\n",
      "   macro avg       0.69      0.80      0.65       334\n",
      "weighted avg       0.88      0.68      0.71       334\n",
      "\n",
      "dart 7 0.6826347305389222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75       266\n",
      "           1       0.39      1.00      0.56        68\n",
      "\n",
      "    accuracy                           0.68       334\n",
      "   macro avg       0.70      0.80      0.66       334\n",
      "weighted avg       0.88      0.68      0.71       334\n",
      "\n",
      "dart 8 0.6826347305389222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75       266\n",
      "           1       0.39      1.00      0.56        68\n",
      "\n",
      "    accuracy                           0.68       334\n",
      "   macro avg       0.70      0.80      0.66       334\n",
      "weighted avg       0.88      0.68      0.71       334\n",
      "\n",
      "dart 9 0.6856287425149701\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.61      0.75       266\n",
      "           1       0.39      1.00      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.70      0.80      0.66       334\n",
      "weighted avg       0.88      0.69      0.72       334\n",
      "\n",
      "dart 10 0.6826347305389222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75       266\n",
      "           1       0.39      1.00      0.56        68\n",
      "\n",
      "    accuracy                           0.68       334\n",
      "   macro avg       0.70      0.80      0.66       334\n",
      "weighted avg       0.88      0.68      0.71       334\n",
      "\n",
      "dart 11 0.688622754491018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.61      0.76       266\n",
      "           1       0.40      1.00      0.57        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.70      0.80      0.66       334\n",
      "weighted avg       0.88      0.69      0.72       334\n",
      "\n",
      "dart 12 0.6826347305389222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75       266\n",
      "           1       0.39      1.00      0.56        68\n",
      "\n",
      "    accuracy                           0.68       334\n",
      "   macro avg       0.70      0.80      0.66       334\n",
      "weighted avg       0.88      0.68      0.71       334\n",
      "\n",
      "dart 13 0.6826347305389222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.75       266\n",
      "           1       0.39      0.99      0.56        68\n",
      "\n",
      "    accuracy                           0.68       334\n",
      "   macro avg       0.69      0.80      0.66       334\n",
      "weighted avg       0.87      0.68      0.71       334\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dart 14 0.6856287425149701\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76       266\n",
      "           1       0.39      0.97      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.79      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "dart 15 0.688622754491018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.62      0.76       266\n",
      "           1       0.39      0.97      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.79      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "dart 16 0.688622754491018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76       266\n",
      "           1       0.39      0.99      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.80      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "dart 17 0.688622754491018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76       266\n",
      "           1       0.39      0.99      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.80      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "dart 18 0.688622754491018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76       266\n",
      "           1       0.39      0.99      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.80      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "dart 19 0.6856287425149701\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76       266\n",
      "           1       0.39      0.97      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.79      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "dart 20 0.688622754491018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.62      0.76       266\n",
      "           1       0.39      0.96      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.79      0.66       334\n",
      "weighted avg       0.86      0.69      0.72       334\n",
      "\n",
      "dart 21 0.6856287425149701\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76       266\n",
      "           1       0.39      0.97      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.79      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "dart 22 0.6946107784431138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.62      0.76       266\n",
      "           1       0.40      0.99      0.57        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.70      0.80      0.67       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "dart 23 0.688622754491018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76       266\n",
      "           1       0.39      0.99      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.80      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "dart 24 0.6916167664670658\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.61      0.76       266\n",
      "           1       0.40      1.00      0.57        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.70      0.81      0.66       334\n",
      "weighted avg       0.88      0.69      0.72       334\n",
      "\n",
      "dart 25 0.688622754491018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76       266\n",
      "           1       0.39      0.99      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.80      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "dart 26 0.6856287425149701\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76       266\n",
      "           1       0.39      0.99      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.80      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "dart 27 0.6916167664670658\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.62      0.76       266\n",
      "           1       0.40      0.97      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.80      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "dart 28 0.688622754491018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76       266\n",
      "           1       0.39      0.99      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.80      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n",
      "dart 29 0.688622754491018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76       266\n",
      "           1       0.39      0.99      0.56        68\n",
      "\n",
      "    accuracy                           0.69       334\n",
      "   macro avg       0.69      0.80      0.66       334\n",
      "weighted avg       0.87      0.69      0.72       334\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-498f960f663d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores_imbal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "scores_imbal = []\n",
    "boosters = ['gbtree', 'gblinear', 'dart']\n",
    "for booster in boosters:\n",
    "    for i in range(0, 30, 1):\n",
    "        model=xgb.XGBClassifier(booster = booster, random_state=1, learning_rate=(i*0.01))\n",
    "        model.fit(X_train_imbal, y_train_imbal)\n",
    "        scores_resampled.append([model.score(X_test,y_test)])\n",
    "        print(booster, i, model.score(X_test,y_test))\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(classification_report(y_test,y_pred))\n",
    "max(scores_imbal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=xgb.XGBClassifier(booster = 'gbtree', random_state=1,learning_rate=0.22)\n",
    "model.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the new result\n",
    "M3_sp_XGBoost=0.85\n",
    "M3_se_XGBoost=0.75\n",
    "M3_acc_XGBoost=0.83\n",
    "# this is the allowed error:\n",
    "M3_err_sp=0.0692\n",
    "M3_err_se=0.00\n",
    "M3_err_acc=0.057\n",
    "# this is the reference result:\n",
    "M3_sp=0.6\n",
    "M3_se=1.00\n",
    "M3_acc=0.68\n",
    "# this is the gain or loss: new result minus reference result\n",
    "M3_sp_XGBoost_eval=(M3_sp_XGBoost-M3_sp)\n",
    "M3_se_XGBoost_eval=(M3_se_XGBoost-M3_se)\n",
    "M3_acc_XGBoost_eval=(M3_acc_XGBoost-M3_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({ 'Model' : pd.Categorical([\"M1:XGBoost\",\"M2:XGBoost\",\"M2_2:XGBoost\",\"M3:XGBoost\"]),\n",
    "                    'Sp_new_res' : [M1_sp_XGBoost,M2_sp_XGBoost,M2_2_sp_XGBoost,M3_sp_XGBoost],\n",
    "                    'Sp_allowed_err' : [M1_err_sp,M2_err_sp,M2_2_err_sp,M3_err_sp],\n",
    "                    'Sp_ref_result' : [M1_sp,M2_sp,M2_2_sp,M3_sp],\n",
    "                    'Sp_gain/loss' : [M1_sp_XGBoost_eval,M2_sp_XGBoost_eval,M2_2_sp_XGBoost_eval,M3_sp_XGBoost_eval],\n",
    "                    'Se_new_res' : [M1_se_XGBoost,M2_se_XGBoost,M2_2_se_XGBoost,M3_se_XGBoost],\n",
    "                    'Se_allowed_err' : [M1_err_se,M2_err_se,M2_2_err_se,M3_err_se],\n",
    "                    'Se_ref_result' : [M1_se,M2_se,M2_2_se,M3_se],\n",
    "                    'Se_gain/loss' : [M1_se_XGBoost_eval,M2_se_XGBoost_eval,M2_2_se_XGBoost_eval,M3_se_XGBoost_eval],\n",
    "                    'Acc_new_res' : [M1_acc_XGBoost,M2_acc_XGBoost,M2_2_acc_XGBoost,M3_acc_XGBoost],\n",
    "                    'Acc_allowed_err' : [M1_err_acc,M2_err_acc,M2_2_err_acc,M3_err_acc],\n",
    "                    'Acc_ref_result' : [M1_acc,M2_acc,M2_2_acc,M3_acc],\n",
    "                    'Acc_gain/loss' : [M1_acc_XGBoost_eval,M2_acc_XGBoost_eval,M2_2_acc_XGBoost_eval,M3_acc_XGBoost_eval], })\n",
    "results\n",
    "## it is more important to find a model with higher sensitivity and \n",
    "## reasonable specificity for the classification of BCF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
